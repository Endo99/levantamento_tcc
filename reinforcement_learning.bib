Wellington Joji Endo

@article{10.1145/3317572,
author = {Ning, Zhaolong and Dong, Peiran and Wang, Xiaojie and Rodrigues, Joel J. P. C. and Xia, Feng},
title = {Deep Reinforcement Learning for Vehicular Edge Computing: An Intelligent Offloading System},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3317572},
doi = {10.1145/3317572},
abstract = {The development of smart vehicles brings drivers and passengers a comfortable and safe environment. Various emerging applications are promising to enrich users’ traveling experiences and daily life. However, how to execute computing-intensive applications on resource-constrained vehicles still faces huge challenges. In this article, we construct an intelligent offloading system for vehicular edge computing by leveraging deep reinforcement learning. First, both the communication and computation states are modelled by finite Markov chains. Moreover, the task scheduling and resource allocation strategy is formulated as a joint optimization problem to maximize users’ Quality of Experience (QoE). Due to its complexity, the original problem is further divided into two sub-optimization problems. A two-sided matching scheme and a deep reinforcement learning approach are developed to schedule offloading requests and allocate network resources, respectively. Performance evaluations illustrate the effectiveness and superiority of our constructed system.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {60},
numpages = {24},
keywords = {deep reinforcement learning, intelligent offloading, Vehicular system, edge computing}
}

@article{10.1145/3358230,
author = {Tran, Hoang-Dung and Cai, Feiyang and Diego, Manzanas Lopez and Musau, Patrick and Johnson, Taylor T. and Koutsoukos, Xenofon},
title = {Safety Verification of Cyber-Physical Systems with Reinforcement Learning Control},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {5s},
issn = {1539-9087},
url = {https://doi.org/10.1145/3358230},
doi = {10.1145/3358230},
abstract = {This paper proposes a new forward reachability analysis approach to verify safety of cyber-physical systems (CPS) with reinforcement learning controllers. The foundation of our approach lies on two efficient, exact and over-approximate reachability algorithms for neural network control systems using star sets, which is an efficient representation of polyhedra. Using these algorithms, we determine the initial conditions for which a safety-critical system with a neural network controller is safe by incrementally searching a critical initial condition where the safety of the system cannot be established. Our approach produces tight over-approximation error and it is computationally efficient, which allows the application to practical CPS with learning enable components (LECs). We implement our approach in NNV, a recent verification tool for neural networks and neural network control systems, and evaluate its advantages and applicability by verifying safety of a practical Advanced Emergency Braking System (AEBS) with a reinforcement learning (RL) controller trained using the deep deterministic policy gradient (DDPG) method. The experimental results show that our new reachability algorithms are much less conservative than existing polyhedra-based approaches. We successfully determine the entire region of the initial conditions of the AEBS with the RL controller such that the safety of the system is guaranteed, while a polyhedra-based approach cannot prove the safety properties of the system.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = oct,
articleno = {105},
numpages = {22},
keywords = {reinforcement learning, verification, Formal methods}
}

@article{10.5555/3291125.3291134,
author = {De Bruin, Tim and Kober, Jens and Tuyls, Karl and Babu\v{s}ka, Robert},
title = {Experience Selection in Deep Reinforcement Learning for Control},
year = {2018},
issue_date = {January 2018},
publisher = {JMLR.org},
volume = {19},
number = {1},
issn = {1532-4435},
abstract = {Experience replay is a technique that allows off-policy reinforcement-learning methods to reuse past experiences. The stability and speed of convergence of reinforcement learning, as well as the eventual performance of the learned policy, are strongly dependent on the experiences being replayed. Which experiences are replayed depends on two important choices. The first is which and how many experiences to retain in the experience replay buffer. The second choice is how to sample the experiences that are to be replayed from that buffer. We propose new methods for the combined problem of experience retention and experience sampling. We refer to the combination as experience selection. We focus our investigation specifically on the control of physical systems, such as robots, where exploration is costly. To determine which experiences to keep and which to replay, we investigate different proxies for their immediate and long-term utility. These proxies include age, temporal difference error and the strength of the applied exploration noise. Since no currently available method works in all situations, we propose guidelines for using prior knowledge about the characteristics of the control problem at hand to choose the appropriate experience replay strategy.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {347–402},
numpages = {56},
keywords = {robotics, experience replay, control, reinforcement learning, deep learning}
}

@article{10.1145/2897824.2925881,
author = {Peng, Xue Bin and Berseth, Glen and van de Panne, Michiel},
title = {Terrain-Adaptive Locomotion Skills Using Deep Reinforcement Learning},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/2897824.2925881},
doi = {10.1145/2897824.2925881},
abstract = {Reinforcement learning offers a promising methodology for developing skills for simulated characters, but typically requires working with sparse hand-crafted features. Building on recent progress in deep reinforcement learning (DeepRL), we introduce a mixture of actor-critic experts (MACE) approach that learns terrain-adaptive dynamic locomotion skills using high-dimensional state and terrain descriptions as input, and parameterized leaps or steps as output actions. MACE learns more quickly than a single actor-critic approach and results in actor-critic experts that exhibit specialization. Additional elements of our solution that contribute towards efficient learning include Boltzmann exploration and the use of initial actor biases to encourage specialization. Results are demonstrated for multiple planar characters and terrain classes.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {81},
numpages = {12},
keywords = {reinforcement learning, physics-based characters}
}

@article{10.5555/3122009.3208017,
author = {Wirth, Christian and Akrour, Riad and Neumann, Gerhard and F\"{u}rnkranz, Johannes},
title = {A Survey of Preference-Based Reinforcement Learning Methods},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {Reinforcement learning (RL) techniques optimize the accumulated long-term reward of a suitably chosen reward function. However, designing such a reward function often requires a lot of task-specific prior knowledge. The designer needs to consider different objectives that do not only influence the learned behavior but also the learning progress. To alleviate these issues, preference-based reinforcement learning algorithms (PbRL) have been proposed that can directly learn from an expert's preferences instead of a hand-designed numeric reward. PbRL has gained traction in recent years due to its ability to resolve the reward shaping problem, its ability to learn from non numeric rewards and the possibility to reduce the dependence on expert knowledge. We provide a unified framework for PbRL that describes the task formally and points out the different design principles that affect the evaluation task for the human as well as the computational complexity. The design principles include the type of feedback that is assumed, the representation that is learned to capture the preferences, the optimization problem that has to be solved as well as how the exploration/exploitation problem is tackled. Furthermore, we point out shortcomings of current algorithms, propose open research questions and briefly survey practical tasks that have been solved using PbRL.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {4945–4990},
numpages = {46},
keywords = {policy search, Markov decision process, reinforcement learning, temporal difference learning, qualitative feedback, preference-based reinforcement learning, preference learning}
}

@article{10.1109/TASLP.2018.2851664,
author = {Weisz, Gellert and Budzianowski, Pawel and Su, Pei-Hao and Gasic, Milica},
title = {Sample Efficient Deep Reinforcement Learning for Dialogue Systems With Large Action Spaces},
year = {2018},
issue_date = {November 2018},
publisher = {IEEE Press},
volume = {26},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2851664},
doi = {10.1109/TASLP.2018.2851664},
abstract = {In spoken dialogue systems, we aim to deploy artificial intelligence to build automated dialogue agents that can converse with humans. A part of this effort is the policy optimization task, which attempts to find a policy describing how to respond to humans, in the form of a function taking the current state of the dialogue and returning the response of the system. In this paper, we investigate deep reinforcement learning approaches to solve this problem. Particular attention is given to actor-critic methods, off-policy reinforcement learning with experience replay, and various methods aimed at reducing the bias and variance of estimators. When combined, these methods result in the previously proposed ACER algorithm that gave competitive results in gaming environments. These environments, however, are fully observable and have a relatively small action set so, in this paper, we examine the application of ACER to dialogue policy optimization. We show that this method beats the current state of the art in deep learning approaches for spoken dialogue systems. This not only leads to a more sample efficient algorithm that can train faster, but also allows us to apply the algorithm in more difficult environments than before. We thus experiment with learning in a very large action space, which has two orders of magnitude more actions than previously considered. We find that ACER trains significantly faster than the current state of the art.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2083–2097},
numpages = {15}
}

@article{10.1145/3414840,
author = {Ren, Hongshuai and Wang, Yang and Xu, Chengzhong and Chen, Xi},
title = {SMig-RL: An Evolutionary Migration Framework for Cloud Services Based on Deep Reinforcement Learning},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {1533-5399},
url = {https://doi.org/10.1145/3414840},
doi = {10.1145/3414840},
abstract = {Service migration is an often-used approach in cloud computing to minimize the access cost by moving the service close to most users. Although it is effective in a certain sense, the service migration in existing research still suffers from some deficiencies in its evolutionary abilities in scalability, sensitivity, and adaptability to effectively react to the dynamically changing environments. This article proposes an evolutionary framework based on deep reinforcement learning for virtual service migration in large-scale mobile cloud centers. To enhance the spatio-temporal sensitivity of the algorithm, we design a scalable reward function for virtual service migration, redefine the input state, and add a Recurrent Neural Network (RNN) to the learning framework. Additionally, in order to enhance the adaptability of the algorithm, we also decompose the action space and exploit the network cost to adjust the number of virtual machine (VMs). The experimental results show that, compared with the existing results, the migration strategy generated by the algorithm can not only significantly reduce the total service cost and achieve the load balancing at the same time, but also address the burst situations with low cost in dynamic environments.},
journal = {ACM Trans. Internet Technol.},
month = oct,
articleno = {43},
numpages = {18},
keywords = {Cloud computing, dynamic service migration, RNN, deep reinforcement learning, Q-learning, mobile access}
}

@article{10.1145/3291045,
author = {Mendon\c{C}a, Matheus R. F. and Ziviani, Artur and Barreto, Andr\'{E} M. S.},
title = {Graph-Based Skill Acquisition For Reinforcement Learning},
year = {2019},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3291045},
doi = {10.1145/3291045},
abstract = {In machine learning, Reinforcement Learning (RL) is an important tool for creating intelligent agents that learn solely through experience. One particular subarea within the RL domain that has received great attention is how to define macro-actions, which are temporal abstractions composed of a sequence of primitive actions. This subarea, loosely called skill acquisition, has been under development for several years and has led to better results in a diversity of RL problems. Among the many skill acquisition approaches, graph-based methods have received considerable attention. This survey presents an overview of graph-based skill acquisition methods for RL. We cover a diversity of these approaches and discuss how they evolved throughout the years. Finally, we also discuss the current challenges and open issues in the area of graph-based skill acquisition for RL.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {6},
numpages = {26},
keywords = {reinforcement learning, clustering, Skill acquisition, graph analytics, centrality}
}

@article{10.1145/3291045,
author = {Mendon\c{C}a, Matheus R. F. and Ziviani, Artur and Barreto, Andr\'{E} M. S.},
title = {Graph-Based Skill Acquisition For Reinforcement Learning},
year = {2019},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3291045},
doi = {10.1145/3291045},
abstract = {In machine learning, Reinforcement Learning (RL) is an important tool for creating intelligent agents that learn solely through experience. One particular subarea within the RL domain that has received great attention is how to define macro-actions, which are temporal abstractions composed of a sequence of primitive actions. This subarea, loosely called skill acquisition, has been under development for several years and has led to better results in a diversity of RL problems. Among the many skill acquisition approaches, graph-based methods have received considerable attention. This survey presents an overview of graph-based skill acquisition methods for RL. We cover a diversity of these approaches and discuss how they evolved throughout the years. Finally, we also discuss the current challenges and open issues in the area of graph-based skill acquisition for RL.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {6},
numpages = {26},
keywords = {Skill acquisition, reinforcement learning, graph analytics, clustering, centrality}
}

@ARTICLE{9216601,
author={Z. {Zhang} and D. {Wang} and J. {Gao}},
journal={IEEE Transactions on Neural Networks and Learning Systems},
title={Learning Automata-Based Multiagent Reinforcement Learning for Optimization of Cooperative Tasks},
year={2020},
volume={},
number={},
pages={1-14},
doi={10.1109/TNNLS.2020.3025711}}

@INPROCEEDINGS{8969579,
author={W. -x. {Liu}},
booktitle={2019 IEEE Symposium on Computers and Communications (ISCC)},
title={Intelligent Routing based on Deep Reinforcement Learning in Software-Defined Data-Center Networks},
year={2019},
volume={},
number={},
pages={1-6},
doi={10.1109/ISCC47284.2019.8969579}}

@INPROCEEDINGS{8785463,
author={R. {Tan} and J. {Zhou} and H. {Du} and S. {Shang} and L. {Dai}},
booktitle={2019 IEEE 8th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)},
title={An modeling processing method for video games based on deep reinforcement learning},
year={2019},
volume={},
number={},
pages={939-942},
doi={10.1109/ITAIC.2019.8785463}}

@INPROCEEDINGS{8666545,
author={A. {Jeerige} and D. {Bein} and A. {Verma}},
booktitle={2019 IEEE 9th Annual Computing and Communication Workshop and Conference (CCWC)},
title={Comparison of Deep Reinforcement Learning Approaches for Intelligent Game Playing},
year={2019},
volume={},
number={},
pages={0366-0371},
doi={10.1109/CCWC.2019.8666545}}

@article{10.1145/3285029,
author = {Zhang, Shuai and Yao, Lina and Sun, Aixin and Tay, Yi},
title = {Deep Learning Based Recommender System: A Survey and New Perspectives},
year = {2019},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3285029},
doi = {10.1145/3285029},
abstract = {With the growing volume of online information, recommender systems have been an effective strategy to overcome information overload. The utility of recommender systems cannot be overstated, given their widespread adoption in many web applications, along with their potential impact to ameliorate many problems related to over-choice. In recent years, deep learning has garnered considerable interest in many research fields such as computer vision and natural language processing, owing not only to stellar performance but also to the attractive property of learning feature representations from scratch. The influence of deep learning is also pervasive, recently demonstrating its effectiveness when applied to information retrieval and recommender systems research. The field of deep learning in recommender system is flourishing. This article aims to provide a comprehensive review of recent research efforts on deep learning-based recommender systems. More concretely, we provide and devise a taxonomy of deep learning-based recommendation models, along with a comprehensive summary of the state of the art. Finally, we expand on current trends and provide new perspectives pertaining to this new and exciting development of the field.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {5},
numpages = {38},
keywords = {deep learning, survey, Recommender system}
}

@article{10.1145/3414472,
author = {Scurto, Hugo and Kerrebroeck, Bavo Van and Caramiaux, Baptiste and Bevilacqua, Fr\'{e}d\'{e}ric},
title = {Designing Deep Reinforcement Learning for Human Parameter Exploration},
year = {2021},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {1},
issn = {1073-0516},
url = {https://doi.org/10.1145/3414472},
doi = {10.1145/3414472},
abstract = {Software tools for generating digital sound often present users with high-dimensional, parametric interfaces, that may not facilitate exploration of diverse sound designs. In this article, we propose to investigate artificial agents using deep reinforcement learning to explore parameter spaces in partnership with users for sound design. We describe a series of user-centred studies to probe the creative benefits of these agents and adapting their design to exploration. Preliminary studies observing users’ exploration strategies with parametric interfaces and testing different agent exploration behaviours led to the design of a fully-functioning prototype, called Co-Explorer, that we evaluated in a workshop with professional sound designers. We found that the Co-Explorer enables a novel creative workflow centred on human–machine partnership, which has been positively received by practitioners. We also highlight varied user exploration behaviours throughout partnering with our system. Finally, we frame design guidelines for enabling such co-exploration workflow in creative digital applications.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = jan,
articleno = {1},
numpages = {35},
keywords = {audio/video, Interaction design, machine learning}
}

@INPROCEEDINGS{9272245,
author={Z. {Tan} and M. {Karakose}},
booktitle={2020 IEEE International Symposium on Systems Engineering (ISSE)},
title={Optimized Deep Reinforcement Learning Approach for Dynamic System},
year={2020},
volume={},
number={},
pages={1-4},
doi={10.1109/ISSE49799.2020.9272245}}

@ARTICLE{8277160,
author={M. {Mahmud} and M. S. {Kaiser} and A. {Hussain} and S. {Vassanelli}},
journal={IEEE Transactions on Neural Networks and Learning Systems},
title={Applications of Deep Learning and Reinforcement Learning to Biological Data},
year={2018},
volume={29},
number={6},
pages={2063-2079},
doi={10.1109/TNNLS.2018.2790388}}

@INPROCEEDINGS{8729310,
author={M. {Wang} and L. {Wang} and T. {Yue}},
booktitle={2019 IEEE 3rd Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)},
title={An Application of Continuous Deep Reinforcement Learning Approach to Pursuit-Evasion Differential Game},
year={2019},
volume={},
number={},
pages={1150-1156},
doi={10.1109/ITNEC.2019.8729310}}

@ARTICLE{9314886,
author={I. {Oh} and S. {Rho} and S. {Moon} and S. {Son} and H. {Lee} and J. {Chung}},
journal={IEEE Transactions on Games},
title={Creating Pro-Level AI for a Real-Time Fighting Game Using Deep Reinforcement Learning},
year={2021},
volume={},
number={},
pages={1-1},
doi={10.1109/TG.2021.3049539}}

@INPROCEEDINGS{8729310,
author={M. {Wang} and L. {Wang} and T. {Yue}},
booktitle={2019 IEEE 3rd Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)},
title={An Application of Continuous Deep Reinforcement Learning Approach to Pursuit-Evasion Differential Game},
year={2019},
volume={},
number={},
pages={1150-1156},
doi={10.1109/ITNEC.2019.8729310}}

@INPROCEEDINGS{8729310,
author={M. {Wang} and L. {Wang} and T. {Yue}},
booktitle={2019 IEEE 3rd Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)},
title={An Application of Continuous Deep Reinforcement Learning Approach to Pursuit-Evasion Differential Game},
year={2019},
volume={},
number={},
pages={1150-1156},
doi={10.1109/ITNEC.2019.8729310}}

@INPROCEEDINGS{8785463,
author={R. {Tan} and J. {Zhou} and H. {Du} and S. {Shang} and L. {Dai}},
booktitle={2019 IEEE 8th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)},
title={An modeling processing method for video games based on deep reinforcement learning},
year={2019},
volume={},
number={},
pages={939-942},
doi={10.1109/ITAIC.2019.8785463}}

@INPROCEEDINGS{9036763,
author={F. {Moreno-Vera}},
booktitle={2019 IEEE Latin American Conference on Computational Intelligence (LA-CCI)},
title={Performing Deep Recurrent Double Q-Learning for Atari Games},
year={2019},
volume={},
number={},
pages={1-4},
doi={10.1109/LA-CCI47412.2019.9036763}}

@INPROCEEDINGS{8408296,
author={M. {Lu} and X. {Li}},
booktitle={2018 Chinese Control And Decision Conference (CCDC)},
title={Deep reinforcement learning policy in Hex game system},
year={2018},
volume={},
number={},
pages={6623-6626},
doi={10.1109/CCDC.2018.8408296}}

@INPROCEEDINGS{8666545,
author={A. {Jeerige} and D. {Bein} and A. {Verma}},
booktitle={2019 IEEE 9th Annual Computing and Communication Workshop and Conference (CCWC)},
title={Comparison of Deep Reinforcement Learning Approaches for Intelligent Game Playing},
year={2019},
volume={},
number={},
pages={0366-0371},
doi={10.1109/CCWC.2019.8666545}}

@inbook{10.1145/3417188.3417189,
author = {Zhao, Cong and Xiao, Bing and Zha, Lin},
title = {Incomplete Information Competition Strategy Based on Improved Asynchronous Advantage Actor Critical Model},
year = {2020},
isbn = {9781450375481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417188.3417189},
abstract = {In recent years, game theory has been widely used in the field of deep learning, mainly including intelligent competition strategies of complete information games and incomplete information games. This paper focuses on incomplete information games, and proposes a low-dimensional semantic feature based on category coding and an incomplete information competition strategy based on the improved Asynchronous Advantage Actor-Critic (A3C) network model. First, the A3C network model in deep reinforcement learning is adopted in the competition strategy, and its network structure is improved according to the semantic features based on category coding. The improved A3C model is implemented in parallel by a series of "workers". The "workers" is a new deep learning model structure proposed in this paper. Secondly, this article combines supervised learning and Deep Reinforcement Learning (DRL) to propose a new competitive strategy. Through conducting a large number of real-time experiments with human players on online competitive websites, the comparison with the existing methods in terms of the ratio of winning and losing and the ranking rate, the experimental results indicate the superiority of the new competitive strategy.},
booktitle = {Proceedings of the 2020 4th International Conference on Deep Learning Technologies (ICDLT)},
pages = {32–37},
numpages = {6}
}

@inproceedings{10.1145/3328526.3329634,
author = {Wright, Mason and Wang, Yongzhao and Wellman, Michael P.},
title = {Iterated Deep Reinforcement Learning in Games: History-Aware Training for Improved Stability},
year = {2019},
isbn = {9781450367929},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328526.3329634},
doi = {10.1145/3328526.3329634},
abstract = {Deep reinforcement learning (RL) is a powerful method for generating policies in complex environments, and recent breakthroughs in game-playing have leveraged deep RL as part of an iterative multiagent search process. We build on such developments and present an approach that learns progressively better mixed strategies in complex dynamic games of imperfect information, through iterated use of empirical game-theoretic analysis (EGTA) with deep RL policies. We apply the approach to a challenging cybersecurity game defined over attack graphs. Iterating deep RL with EGTA to convergence over dozens of rounds, we generate mixed strategies far stronger than earlier published heuristic strategies for this game. We further refine the strategy-exploration process, by fine-tuning in a training environment that includes out-of-equilibrium but recently seen opponents. Experiments suggest this history-aware approach yields strategies with lower regret at each stage of training.},
booktitle = {Proceedings of the 2019 ACM Conference on Economics and Computation},
pages = {617–636},
numpages = {20},
keywords = {double oracle, deep reinforcement learning, multi-agent reinforcement learning, attack graphs, security games},
location = {Phoenix, AZ, USA},
series = {EC '19}
}

@inproceedings{10.1145/3278721.3278776,
author = {Iyer, Rahul and Li, Yuezhang and Li, Huao and Lewis, Michael and Sundar, Ramitha and Sycara, Katia},
title = {Transparency and Explanation in Deep Reinforcement Learning Neural Networks},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278776},
doi = {10.1145/3278721.3278776},
abstract = {Autonomous AI systems will be entering human society in the near future to provide services and work alongside humans. For those systems to be accepted and trusted, the users should be able to understand the reasoning process of the system, i.e. the system should be transparent. System transparency enables humans to form coherent explanations of the system's decisions and actions. Transparency is important not only for user trust, but also for software debugging and certification. In recent years, Deep Neural Networks have made great advances in multiple application areas. However, deep neural networks are opaque. In this paper, we report on work in transparency in Deep Reinforcement Learning Networks (DRLN). Such networks have been extremely successful in accurately learning action control in image input domains, such as Atari games. In this paper, we propose a novel and general method that (a) incorporates explicit object recognition processing into deep reinforcement learning models, (b) forms the basis for the development of "object saliency maps", to provide visualization of internal states of DRLNs, thus enabling the formation of explanations and (c) can be incorporated in any existing deep reinforcement learning framework. We present computational results and human experiments to evaluate our approach.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {144–150},
numpages = {7},
keywords = {explainable ai, deep reinforcement learning, system transparency, human factors, human-ai interaction},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3377929.3398093,
author = {Waris, Faisal and Reynolds, Robert},
title = {Neuro-Evolution Using Game-Driven Cultural Algorithms},
year = {2020},
isbn = {9781450371278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377929.3398093},
doi = {10.1145/3377929.3398093},
abstract = {Contemporary 'deep learning' (DL) models have proven to be effective in a wide variety of applications. However, the right network topology for the problem at hand may be complex and not immediately obvious. This has given rise to the secondary field of neural architecture search (NAS). This paper describes a NAS method based on graph evolution pioneered by Neuro-evolution of Augmenting Topologies (NEAT), but driven by the evolutionary mechanisms underlying Cultural Algorithms (CA). CA is a population-based, stochastic optimization system inspired by problem solving in human cultures, suited to solving problems such as NAS. We present CATNeuro a system for evolving DL models guided by CA metaheuristics called Knowledge Sources (KS). The KS store knowledge harvested from prior generations and use it to guide subsequent generations in the search space. A knowledge distribution mechanism, which assigns a KS to each individual in the population, is an instrumental part of this process. CATNeuro, is applied to find optimal network topologies to play a 2D fighting game called FightingICE (based on "The Rumble Fish" game). A policy-based, reinforcement learning method is used to create the training data for network optimization. CATNeuro is still evolving. In this primary foray into NAS, we contrast the performance of CATNeuro with two different knowledge distribution mechanisms - the stalwart Weighted Majority (WTD) - which represents "wisdom of the crowds" - and a new one based on the Stag-Hunt game from evolutionary game theory. We show that Stag-Hunt has a statistical edge over WTD in many areas and thus is a better candidate for future development with CATNeuro.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion},
pages = {1857–1865},
numpages = {9},
keywords = {game theory, neural architecture search, cultural algorithms, ACM proceedings, reinforcement learning},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@inproceedings{10.1145/3377929.3398093,
author = {Waris, Faisal and Reynolds, Robert},
title = {Neuro-Evolution Using Game-Driven Cultural Algorithms},
year = {2020},
isbn = {9781450371278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377929.3398093},
doi = {10.1145/3377929.3398093},
abstract = {Contemporary 'deep learning' (DL) models have proven to be effective in a wide variety of applications. However, the right network topology for the problem at hand may be complex and not immediately obvious. This has given rise to the secondary field of neural architecture search (NAS). This paper describes a NAS method based on graph evolution pioneered by Neuro-evolution of Augmenting Topologies (NEAT), but driven by the evolutionary mechanisms underlying Cultural Algorithms (CA). CA is a population-based, stochastic optimization system inspired by problem solving in human cultures, suited to solving problems such as NAS. We present CATNeuro a system for evolving DL models guided by CA metaheuristics called Knowledge Sources (KS). The KS store knowledge harvested from prior generations and use it to guide subsequent generations in the search space. A knowledge distribution mechanism, which assigns a KS to each individual in the population, is an instrumental part of this process. CATNeuro, is applied to find optimal network topologies to play a 2D fighting game called FightingICE (based on "The Rumble Fish" game). A policy-based, reinforcement learning method is used to create the training data for network optimization. CATNeuro is still evolving. In this primary foray into NAS, we contrast the performance of CATNeuro with two different knowledge distribution mechanisms - the stalwart Weighted Majority (WTD) - which represents "wisdom of the crowds" - and a new one based on the Stag-Hunt game from evolutionary game theory. We show that Stag-Hunt has a statistical edge over WTD in many areas and thus is a better candidate for future development with CATNeuro.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion},
pages = {1857–1865},
numpages = {9},
keywords = {ACM proceedings, neural architecture search, game theory, cultural algorithms, reinforcement learning},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@inproceedings{10.1145/3194085.3194088,
author = {Spryn, Mitchell and Sharma, Aditya and Parkar, Dhawal and Shrimal, Madhur},
title = {Distributed Deep Reinforcement Learning on the Cloud for Autonomous Driving},
year = {2018},
isbn = {9781450357395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194085.3194088},
doi = {10.1145/3194085.3194088},
abstract = {This paper proposes an architecture for leveraging cloud computing technology to reduce training time for deep reinforcement learning models for autonomous driving by distributing the training process across a pool of virtual machines. By parallelizing the training process, careful design of the reward function and use of techniques like transfer learning, we demonstrate a decrease in training time for our example autonomous driving problem from 140 hours to less than 1 hour. We go over our network architecture, job distribution paradigm, reward function design and report results from experiments on small sized cluster (1--6 training nodes) of machines. We also discuss the limitations of our approach when trying to scale up to massive clusters.},
booktitle = {Proceedings of the 1st International Workshop on Software Engineering for AI in Autonomous Systems},
pages = {16–22},
numpages = {7},
keywords = {distributed machine learning, cloud computing, simulation, deep reinforcement learning, autonomous driving},
location = {Gothenburg, Sweden},
series = {SEFAIS '18}
}

@inproceedings{10.1145/3029798.3038399,
author = {Shih, Victor and Jangraw, David and Saproo, Sameer and Sajda, Paul},
title = {Deep Reinforcement Learning Using Neurophysiological Signatures of Interest},
year = {2017},
isbn = {9781450348850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3029798.3038399},
doi = {10.1145/3029798.3038399},
abstract = {We present a study where human neurophysiological signals are used as implicit feedback to alter the behavior of a deep learning based autonomous driving agent in a simulated virtual environment.},
booktitle = {Proceedings of the Companion of the 2017 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {285–286},
numpages = {2},
keywords = {bci, deep reinforcement learning, autonomous vehicles, affective computing},
location = {Vienna, Austria},
series = {HRI '17}
}

@inbook{10.1145/3394486.3403231,
author = {Siddique, A. B. and Oymak, Samet and Hristidis, Vagelis},
title = {Unsupervised Paraphrasing via Deep Reinforcement Learning},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403231},
abstract = {Paraphrasing is expressing the meaning of an input sentence in different wording while maintaining fluency (i.e., grammatical and syntactical correctness). Most existing work on paraphrasing use supervised models that are limited to specific domains (e.g., image captions). Such models can neither be straightforwardly transferred to other domains nor generalize well, and creating labeled training data for new domains is expensive and laborious. The need for paraphrasing across different domains and the scarcity of labeled training data in many such domains call for exploring unsupervised paraphrase generation methods. We propose Progressive Unsupervised Paraphrasing (PUP): a novel unsupervised paraphrase generation method based on deep reinforcement learning (DRL). PUP uses a variational autoencoder (trained using a non-parallel corpus) to generate a seed paraphrase that warm-starts the DRL model. Then, PUP progressively tunes the seed paraphrase guided by our novel reward function which combines semantic adequacy, language fluency, and expression diversity measures to quantify the quality of the generated paraphrases in each iteration without needing parallel sentences. Our extensive experimental evaluation shows that PUP outperforms unsupervised state-of-the-art paraphrasing techniques in terms of both automatic metrics and user studies on four real datasets. We also show that PUP outperforms domain-adapted supervised algorithms on several datasets. Our evaluation also shows that PUP achieves a great trade-off between semantic similarity and diversity of expression.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1800–1809},
numpages = {10}
}

@inproceedings{10.5555/3306127.3331700,
author = {Subramanian, Jayakumar and Mahajan, Aditya},
title = {Reinforcement Learning in Stationary Mean-Field Games},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multi-agent reinforcement learning has made significant progress in recent years, but it remains a hard problem. Hence, one often resorts to developing learning algorithms for specific classes of multi-agent systems. In this paper we study reinforcement learning in a specific class of multi-agent systems systems called mean-field games. In particular, we consider learning in stationary mean-field games. We identify two different solution concepts---stationary mean-field equilibrium and stationary mean-field social-welfare optimal policy---for such games based on whether the agents are non-cooperative or cooperative, respectively. We then generalize these solution concepts to their local variants using bounded rationality based arguments. For these two local solution concepts, we present two reinforcement learning algorithms. We show that the algorithms converge to the right solution under mild technical conditions and demonstrate this using two numerical examples.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {251–259},
numpages = {9},
keywords = {stationary mean-field games, multi-agent reinforcement learning, mean-field games, bounded rationality},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@INPROCEEDINGS{8832340,
author={S. {Li} and Y. {Qi} and J. {Bo} and Y. {Fu}},
booktitle={2019 Chinese Control And Decision Conference (CCDC)},
title={Design and Implementation of Surakarta Game System Based on Reinforcement Learning},
year={2019},
volume={},
number={},
pages={6326-6329},
doi={10.1109/CCDC.2019.8832340}}

@INPROCEEDINGS{9070827,
author={A. J. {Almalki} and P. {Wocjan}},
booktitle={2019 International Conference on Computational Science and Computational Intelligence (CSCI)},
title={Exploration of Reinforcement Learning to Play Snake Game},
year={2019},
volume={},
number={},
pages={377-381},
doi={10.1109/CSCI49370.2019.00073}}

@INPROCEEDINGS{8490409,
author={P. {Andersen} and M. {Goodwin} and O. {Granmo}},
booktitle={2018 IEEE Conference on Computational Intelligence and Games (CIG)},
title={Deep RTS: A Game Environment for Deep Reinforcement Learning in Real-Time Strategy Games},
year={2018},
volume={},
number={},
pages={1-8},
doi={10.1109/CIG.2018.8490409}}

@INPROCEEDINGS{9067731,
author={Z. {LI} and P. {KUANG} and T. {ZHANG} and H. {YAN} and X. {GU}},
booktitle={2019 16th International Computer Conference on Wavelet Active Media Technology and Information Processing},
title={Deep Reinforcement Learning Based Game Decision Algorithm for Digital Media Education},
year={2019},
volume={},
number={},
pages={139-142},
doi={10.1109/ICCWAMTIP47768.2019.9067731}}

@INPROCEEDINGS{8833043,
author={Y. {Zhang} and S. {Li} and X. {Xiong}},
booktitle={2019 Chinese Control And Decision Conference (CCDC)},
title={A Study on the Game System of Dots and Boxes Based on Reinforcement Learning},
year={2019},
volume={},
number={},
pages={6319-6322},
doi={10.1109/CCDC.2019.8833043}}

@INPROCEEDINGS{8320266,
author={H. {Singal} and P. {Aggarwal} and V. {Dutt}},
booktitle={2017 International Conference on Machine Learning and Data Science (MLDS)},
title={Modeling Decisions in Games Using Reinforcement Learning},
year={2017},
volume={},
number={},
pages={98-105},
doi={10.1109/MLDS.2017.13}}

@INPROCEEDINGS{8636940,
author={P. B. S. {Serafim} and Y. L. B. {Nogueira} and C. A. {Vidal} and J. B. C. {Neto}},
booktitle={2018 17th Brazilian Symposium on Computer Games and Digital Entertainment (SBGames)},
title={Evaluating Competition in Training of Deep Reinforcement Learning Agents in First-Person Shooter Games},
year={2018},
volume={},
number={},
pages={117-11709},
doi={10.1109/SBGAMES.2018.00023}}

@INPROCEEDINGS{9277188,
author={J. {Hu} and F. {Zhao} and J. {Meng} and S. {Wu}},
booktitle={2020 IEEE International Conference on Information Technology,Big Data and Artificial Intelligence (ICIBA)},
title={Application of Deep Reinforcement Learning in the Board Game},
year={2020},
volume={1},
number={},
pages={809-812},
doi={10.1109/ICIBA50161.2020.9277188}}

@INPROCEEDINGS{9207169,
author={K. {Shao} and Y. {Zhu} and Z. {Tang} and D. {Zhao}},
booktitle={2020 International Joint Conference on Neural Networks (IJCNN)},
title={Cooperative Multi-Agent Deep Reinforcement Learning with Counterfactual Reward},
year={2020},
volume={},
number={},
pages={1-8},
doi={10.1109/IJCNN48605.2020.9207169}}

@INPROCEEDINGS{8400328,
author={P. B. S. {Serafim} and Y. L. B. {Nogueira} and C. {Vidal} and J. {Cavalcante-Neto}},
booktitle={2017 16th Brazilian Symposium on Computer Games and Digital Entertainment (SBGames)},
title={On the Development of an Autonomous Agent for a 3D First-Person Shooter Game Using Deep Reinforcement Learning},
year={2017},
volume={},
number={},
pages={155-163},
doi={10.1109/SBGames.2017.00025}}

@INPROCEEDINGS{8080451,
author={S. {Yoon} and K. {Kim}},
booktitle={2017 IEEE Conference on Computational Intelligence and Games (CIG)},
title={Deep Q networks for visual fighting game AI},
year={2017},
volume={},
number={},
pages={306-308},
doi={10.1109/CIG.2017.8080451}}

@ARTICLE{9049116,
author={D. {Shi} and H. {Gao} and L. {Wang} and M. {Pan} and Z. {Han} and H. V. {Poor}},
journal={IEEE Internet of Things Journal},
title={Mean Field Game Guided Deep Reinforcement Learning for Task Placement in Cooperative Multiaccess Edge Computing},
year={2020},
volume={7},
number={10},
pages={9330-9340},
doi={10.1109/JIOT.2020.2983741}}

@INPROCEEDINGS{8460004,
author={Z. {Wei} and D. {Wang} and M. {Zhang} and A. {Tan} and C. {Miao} and Y. {Zhou}},
booktitle={2018 IEEE International Conference on Agents (ICA)},
title={Autonomous Agents in Snake Game via Deep Reinforcement Learning},
year={2018},
volume={},
number={},
pages={20-25},
doi={10.1109/AGENTS.2018.8460004}}

@INPROCEEDINGS{8451491,
author={Y. {Li} and H. {Chang} and Y. {Lin} and P. {Wu} and Y. F. {Wang}},
booktitle={2018 25th IEEE International Conference on Image Processing (ICIP)},
title={Deep Reinforcement Learning for Playing 2.5D Fighting Games},
year={2018},
volume={},
number={},
pages={3778-3782},
doi={10.1109/ICIP.2018.8451491}}

@INPROCEEDINGS{8588472,
author={T. {Wang} and T. {Kaneko}},
booktitle={2018 Conference on Technologies and Applications of Artificial Intelligence (TAAI)},
title={Application of Deep Reinforcement Learning in Werewolf Game Agents},
year={2018},
volume={},
number={},
pages={28-33},
doi={10.1109/TAAI.2018.00016}}

@INPROCEEDINGS{9188547,
author={Y. {Guo} and Q. {Gao} and F. {Pan}},
booktitle={2020 39th Chinese Control Conference (CCC)},
title={Trained Model Reuse of Autonomous-Driving in Pygame with Deep Reinforcement Learning},
year={2020},
volume={},
number={},
pages={5660-5664},
doi={10.23919/CCC50068.2020.9188547}}

@INPROCEEDINGS{9295872,
author={S. {Liang} and H. {Wan} and T. {Qin} and J. {Li} and W. {Chen}},
booktitle={2020 IEEE 20th International Conference on Communication Technology (ICCT)},
title={Multi-user Computation Offloading for Mobile Edge Computing: A Deep Reinforcement Learning and Game Theory Approach},
year={2020},
volume={},
number={},
pages={1534-1539},
doi={10.1109/ICCT50939.2020.9295872}}

@INPROCEEDINGS{8578287,
author={K. M. {Lee} and H. {Myeong} and G. {Song}},
booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
title={SeedNet: Automatic Seed Generation with Deep Reinforcement Learning for Robust Interactive Segmentation},
year={2018},
volume={},
number={},
pages={1760-1768},
doi={10.1109/CVPR.2018.00189}}

@ARTICLE{9106862,
author={Y. {Liang} and C. {Guo} and Z. {Ding} and H. {Hua}},
journal={IEEE Transactions on Power Systems},
title={Agent-Based Modeling in Electricity Market Using Deep Deterministic Policy Gradient Algorithm},
year={2020},
volume={35},
number={6},
pages={4180-4192},
doi={10.1109/TPWRS.2020.2999536}}

@INPROCEEDINGS{9212868,
author={Y. {Zhang} and T. {Liu} and Y. {Zhu} and Y. {Yang}},
booktitle={2020 IEEE/ACM 28th International Symposium on Quality of Service (IWQoS)},
title={A Deep Reinforcement Learning Approach for Online Computation Offloading in Mobile Edge Computing},
year={2020},
volume={},
number={},
pages={1-10},
doi={10.1109/IWQoS49365.2020.9212868}}

@INPROCEEDINGS{8794389,
author={M. {Shen} and J. P. {How}},
booktitle={2019 International Conference on Robotics and Automation (ICRA)},
title={Active Perception in Adversarial Scenarios using Maximum Entropy Deep Reinforcement Learning},
year={2019},
volume={},
number={},
pages={3384-3390},
doi={10.1109/ICRA.2019.8794389}}

@INPROCEEDINGS{9313387,
author={H. {Wang} and H. {Tang} and J. {Hao} and X. {Hao} and Y. {Fu} and Y. {Ma}},
booktitle={2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)},
title={Large Scale Deep Reinforcement Learning in War-games},
year={2020},
volume={},
number={},
pages={1693-1699},
doi={10.1109/BIBM49941.2020.9313387}}

@INPROCEEDINGS{9311945,
author={W. {Farag}},
booktitle={2020 International Conference on Innovation and Intelligence for Informatics, Computing and Technologies (3ICT)},
title={Multi-Agent Reinforcement Learning using the Deep Distributed Distributional Deterministic Policy Gradients Algorithm},
year={2020},
volume={},
number={},
pages={1-6},
doi={10.1109/3ICT51146.2020.9311945}}

@INPROCEEDINGS{9206918,
author={R. {Yang} and H. {Tang} and B. {Jin}},
booktitle={2020 International Joint Conference on Neural Networks (IJCNN)},
title={Monoceros: A New Approach for Training an Agent to Play FPS Games},
year={2020},
volume={},
number={},
pages={1-8},
doi={10.1109/IJCNN48605.2020.9206918}}

@INPROCEEDINGS{9040728,  author={L. {Chen} and Q. {Gao}},
booktitle={2019 IEEE 10th International Conference on Software Engineering and Service Science (ICSESS)},
title={Application of Deep Reinforcement Learning on Automated Stock Trading},
year={2019},
volume={},
number={},
pages={29-33},
doi={10.1109/ICSESS47205.2019.9040728}}

@inproceedings{10.1145/3349341.3349412,
author = {Shang, Tongfei and Ma, Jianfeng and Han, Kun and Yu, Yuan},
title = {Research on Game Problem Under Incomplete Information Condition Based on Deep Reinforcement Learning},
year = {2019},
isbn = {9781450371506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3349341.3349412},
doi = {10.1145/3349341.3349412},
abstract = {Deep reinforcement learning has been widely used in the military field and applied to many fields, especially in the areas of identification, recommendation, decision-making and so on. This paper analyzes the prediction error of game confrontation by deep reinforcement learning under the condition of incomplete information, and proves the effectiveness of deep reinforcement learning for machine game.},
booktitle = {Proceedings of the 2019 International Conference on Artificial Intelligence and Computer Science},
pages = {255–257},
numpages = {3},
keywords = {Deep reinforcement learning, Game, Incomplete information},
location = {Wuhan, Hubei, China},
series = {AICS 2019}
}

@inproceedings{10.1145/3318265.3318294,
author = {Zhou, Yinda and Liu, Weiming and Li, Bin},
title = {Two-Stage Population Based Training Method for Deep Reinforcement Learning},
year = {2019},
isbn = {9781450366380},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318265.3318294},
doi = {10.1145/3318265.3318294},
abstract = {Deep reinforcement learning (DRL) methods has been widely applied on more and more challenging learning tasks, and achieved excellent performance. However, the efficiency of deep reinforcement learning is notoriously sensitive to their own hyperparameter configuration. The optimization process of deep reinforcement learning is highly dynamic and non-stationary, rather than a simple fitting process. So, its optimal hyperparameter should be adaptively adjusted according to the current learning process, rather than using a fixed set of hyperparameter configurations from beginning to end. DeepMind innovatively proposed a population based training (PBT) method for deep reinforcement learning, which achieved hyperparameter adaptation and made the model better trained. However, we assume that at the early stage when the learning model has little knowledge of the environment, frequent hyperparameter change will not be helpful for the model to learn efficiently, while learning with a reasonable fixed hyperparameter configuration will help the model obtain necessary knowledge as quick as possible, which we consider is more important for reinforcement learning at early stage. In this paper, we verified our hypothesis through experiments, and a Two-Stage Population Based Training (TS-PBT) method is proposed, which is a more efficient population based training method for deep reinforcement learning. Experiments show that at the same computational budget, our TS-PBT method makes the final performance of the model significantly better than the PBT method. TS-PBT achieved 40%, 310%, 2%, 53%, 30% and 38% performance improvement over PBT separately in six test environments.},
booktitle = {Proceedings of the 3rd International Conference on High Performance Compilation, Computing and Communications},
pages = {38–44},
numpages = {7},
keywords = {deep reinforcement learning, PBT, two-stage, hyperparameter adaptation, game},
location = {Xi'an, China},
series = {HP3C '19}
}

@inproceedings{10.5555/3306127.3331943,
author = {Carr, Thomas and Chli, Maria and Vogiatzis, George},
title = {Domain Adaptation for Reinforcement Learning on the Atari},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Deep Reinforcement learning is a powerful machine learning paradigm that has had significant success across a wide range of control problems. This success often requires long training times to achieve. Observing that many problems share similarities, it is likely that much of the training done could be redundant if knowledge could be efficiently and appropriately shared across tasks. In this paper we demonstrate a novel adversarial domain adaptation approach to transfer state knowledge between domains and tasks on the Atari game suite. We show how this approach can successfully transfer across very different visual domains of the Atari platform. We focus on semantically related games that involve returning a ball with the user controlled agent. Our experiments demonstrate that our method reduces the number of samples required to successfully train an agent to play an Atari game.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1859–1861},
numpages = {3},
keywords = {reinforcement learning, domain adapatation, deep learning},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@article{SHANTIA2021103731,
title = {Two-stage visual navigation by deep neural networks and multi-goal reinforcement learning},
journal = {Robotics and Autonomous Systems},
volume = {138},
pages = {103731},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103731},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000166},
author = {Amirhossein Shantia and Rik Timmers and Yiebo Chong and Cornel Kuiper and Francesco Bidoia and Lambert Schomaker and Marco Wiering},
keywords = {Robotic navigation, Reinforcement learning, Deep neural networks, Localization and mapping, Robot simulation},
abstract = {In this paper, we propose a two-stage learning framework for visual navigation in which the experience of the agent during exploration of one goal is shared to learn to navigate to other goals. We train a deep neural network for estimating the robot’s position in the environment using ground truth information provided by a classical localization and mapping approach. The second simpler multi-goal Q-function learns to traverse the environment by using the provided discretized map. Transfer learning is applied to the multi-goal Q-function from a maze structure to a 2D simulator and is finally deployed in a 3D simulator where the robot uses the estimated locations from the position estimator deep network. In the experiments, we first compare different architectures to select the best deep network for location estimation, and then compare the effects of the multi-goal reinforcement learning method to traditional reinforcement learning. The results show a significant improvement when multi-goal reinforcement learning is used. Furthermore, the results of the location estimator show that a deep network can learn and generalize in different environments using camera images with high accuracy in both position and orientation.}
}

@article{KIM2020113695,
title = {Genetic state-grouping algorithm for deep reinforcement learning},
journal = {Expert Systems with Applications},
volume = {161},
pages = {113695},
year = {2020},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.113695},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420305194},
author = {Man-Je Kim and Jun Suk Kim and Sungjin James Kim and Min-jung Kim and Chang Wook Ahn},
keywords = {Reinforcement learning, Genetic algorithm, Hybrid method, Monte Carlo Tree Search, Game AI},
abstract = {Although Reinforcement learning has already been considered one of the most important and well-known techniques of machine learning, its applicability remains limited in the real-world problems due to its long initial learning time and unstable learning. Especially, the problem of an overwhelming number of the branching factors under real-time constraint still stays unconquered, demanding a new method for the next generation of reinforcement learning. In this paper, we propose Genetic State-Grouping Algorithm based on deep reinforcement learning. The core idea is to divide the entire set of states into a few state groups. Each group consists of states that are mutually similar, thus representing their common features. The state groups are then processed with the Genetic Optimizer, which finds outstanding actions. These steps help the Deep Q Network avoid excessive exploration, thereby contributing to the significant reduction of initial learning time. The experiment on the real-time fighting video game (FightingICE) shows the effectiveness of our proposed approach.}
}

@article{ALJAAFREH2019241,
title = {Development of a Computer Player for Seejeh (A.K.A Seega, Siga, Kharbga) Board Game with Deep Reinforcement Learning},
journal = {Procedia Computer Science},
volume = {160},
pages = {241-247},
year = {2019},
note = {The 10th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2019) / The 9th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.463},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919316783},
author = {Ahmad Aljaafreh and Naeem Al-Oudat},
keywords = {Board game, deep reinforcement learning, search, MCTS, Minimax, self-play, Seejeh},
abstract = {Recent years have proven the existing room of deep reinforcement learning (DRL) applications. DRL has been utilized as an AI computer player in many board games. Seejeh is an ancient board game, where no one attempts to create an AI system that is able to learn to play it. Seejeh is a two-player, zero-sum, discrete, finite and deterministic game of perfect information. Seejeh board game is different from all other strategic board games. It has two stages; Positioning and moving. Player place two tiles at each action in stage one. A player might have a sequence of moves in the second stage unlike Othello and Go. In this work, we develop an automated player based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. This paper presents a self-play algorithm utilizing DRL and search algorithms. The system starts with a neural network that knows nothing about the game of Seejeh. It then plays games against itself, by combining this neural network with powerful search algorithms. To the best of our knowledge, we are the first who develop an agent that learns to play Seejeh game.}
}

@article{ATKINSON2021291,
title = {Pseudo-rehearsal: Achieving deep reinforcement learning without catastrophic forgetting},
journal = {Neurocomputing},
volume = {428},
pages = {291-307},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.11.050},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220318439},
author = {Craig Atkinson and Brendan McCane and Lech Szymanski and Anthony Robins},
keywords = {Deep reinforcement learning, Pseudo-rehearsal, Catastrophic forgetting, Generative adversarial network},
abstract = {Neural networks can achieve excellent results in a wide variety of applications. However, when they attempt to sequentially learn, they tend to learn the new task while catastrophically forgetting previous ones. We propose a model that overcomes catastrophic forgetting in sequential reinforcement learning by combining ideas from continual learning in both the image classification domain and the reinforcement learning domain. This model features a dual memory system which separates continual learning from reinforcement learning and a pseudo-rehearsal system that “recalls” items representative of previous tasks via a deep generative network. Our model sequentially learns Atari 2600 games without demonstrating catastrophic forgetting and continues to perform above human level on all three games. This result is achieved without: demanding additional storage requirements as the number of tasks increases, storing raw data or revisiting past tasks. In comparison, previous state-of-the-art solutions are substantially more vulnerable to forgetting on these complex deep reinforcement learning tasks.}
}

@article{PATEL2019108,
title = {Improved robustness of reinforcement learning policies upon conversion to spiking neuronal network platforms applied to Atari Breakout game},
journal = {Neural Networks},
volume = {120},
pages = {108-115},
year = {2019},
note = {special Issue in Honor of the 80th Birthday of Stephen Grossberg},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302266},
author = {Devdhar Patel and Hananel Hazan and Daniel J. Saunders and Hava T. Siegelmann and Robert Kozma},
keywords = {Spiking neural networks, Reinforcement learning, Deep learning, Robustness, Atari},
abstract = {Deep Reinforcement Learning (RL) demonstrates excellent performance on tasks that can be solved by trained policy. It plays a dominant role among cutting-edge machine learning approaches using multi-layer Neural networks (NNs). At the same time, Deep RL suffers from high sensitivity to noisy, incomplete, and misleading input data. Following biological intuition, we involve Spiking Neural Networks (SNNs) to address some deficiencies of deep RL solutions. Previous studies in image classification domain demonstrated that standard NNs (with ReLU nonlinearity) trained using supervised learning can be converted to SNNs with negligible deterioration in performance. In this paper, we extend those conversion results to the domain of Q-Learning NNs trained using RL. We provide a proof of principle of the conversion of standard NN to SNN. In addition, we show that the SNN has improved robustness to occlusion in the input image. Finally, we introduce results with converting full-scale Deep Q-network to SNN, paving the way for future research to robust Deep RL applications.}
}

@article{DING2020102069,
title = {A deep reinforcement learning for user association and power control in heterogeneous networks},
journal = {Ad Hoc Networks},
volume = {102},
pages = {102069},
year = {2020},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2019.102069},
url = {https://www.sciencedirect.com/science/article/pii/S1570870519310546},
author = {Hui Ding and Feng Zhao and Jie Tian and Dongyang Li and Haixia Zhang},
keywords = {Heterogeneous networks, User association, Power control, Reinforcement learning, Deep Q-learning network},
abstract = {Heterogeneous network (HetNet) is a promising solution to satisfy the unprecedented demand for higher data rate in the next generation mobile networks. Different from the traditional single-layer cellular networks, how to provide the best service to the user equipments (UEs) under the limited resource is an urgent problem to solve. In order to efficiently address the above challenge and strive towards high network energy efficiency, the joint optimization problem of user association and power control in orthogonal frequency division multiple access (OFDMA) based uplink HetNets is studied. Considering the non-convex and non-linear characteristics of the problem, a multi-agent deep Q-learning Network (DQN) method is studied to solve the problem. Different from the traditional methods, such as game theory, fractional programming and convex optimization, which need more and accurate network information in practice, the multi-agent DQN method requires less communication information of the environment. Moreover, for the communication environment dynamics, the maximum long-term overall network utility with a new reward function while ensuring the UE’s quality of service (QoS) requirements is achieved by using the multi-agent DQN method. Then, according to the application scenario, the action space, state space and reward function of the multi-agent DQN based framework are redefined and formulated. Simulation results demonstrate that the multi-agent DQN method has the best performance on convergence and energy efficiency compared with the traditional reinforcement learning (Q-learning).}
}

@article{LI2020157,
title = {Accelerating deep reinforcement learning model for game strategy},
journal = {Neurocomputing},
volume = {408},
pages = {157-168},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.06.110},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220303337},
author = {Yifan Li and Yuchun Fang and Zahid Akhtar},
keywords = {Deep reinforcement learning, Convolutional neural network, Depthwise separable convolution, Binary weight network},
abstract = {In recent years, deep reinforcement learning has achieved impressing accuracies in games compared with traditional methods. Prior schemes utilized Convolutional Neural Networks (CNNs) or Long Short-Term Memory networks (LSTMs) to improve the performances of the agents. In this paper, we consider the issue from a different perspective when the training and inference of deep reinforcement learning are required to be performed with limited computing resources. Mainly, we propose two efficient neural network architectures of deep reinforcement learning: Light-Q-Network (LQN) and Binary-Q-Network (BQN). In LQN, The depth-wise separable CNNs are utilized in memory and computation saving. While, in BQN, the weights of convolutional layers are binary that help in shortening the training time and reduce memory consumption. We evaluate our approach on Atari 2600 domain and StarCraft II mini-games. The results demonstratethe efficiency of the proposed architectures. Though performances of agents in most games are still super-human, the proposed methods advance the agent from sub to super-human performance in particular games. Also, we empirically find that non-standard convolution and non-full-precision networks do not affect agent learning game strategy.}
}

@article{BU2019500,
title = {A smart agriculture IoT system based on deep reinforcement learning},
journal = {Future Generation Computer Systems},
volume = {99},
pages = {500-507},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.04.041},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19307277},
author = {Fanyu Bu and Xin Wang},
keywords = {Deep reinforcement learning, Smart agriculture IoT, Edge computing, Cloud computing},
abstract = {Smart agriculture systems based on Internet of Things are the most promising to increase food production and reduce the consumption of resources like fresh water. In this study, we present a smart agriculture IoT system based on deep reinforcement learning which includes four layers, namely agricultural data collection layer, edge computing layer, agricultural data transmission layer, and cloud computing layer. The presented system integrates some advanced information techniques, especially artificial intelligence and cloud computing, with agricultural production to increase food production. Specially, the most advanced artificial intelligence model, deep reinforcement learning is combined in the cloud layer to make immediate smart decisions such as determining the amount of water needed to be irrigated for improving crop growth environment. We present several representative deep reinforcement learning models with their broad applications. Finally, we talk about the open challenges and the potential applications of deep reinforcement learning in smart agriculture IoT systems.}
}

@article{MATULIS2021,
title = {A robot arm digital twin utilising reinforcement learning},
journal = {Computers & Graphics},
year = {2021},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2021.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S009784932100011X},
author = {Marius Matulis and Carlo Harvey},
keywords = {Robot arm, Reinforcement learning, Artificial intelligence, Digital twin},
abstract = {For many industry contexts, the implementation of Artificial Intelligence (AI) has contributed to what has become known as the fourth industrial revolution or “Industry 4.0” and creates an opportunity to deliver significant benefit to both businesses and their stakeholders. Robot arms are one of the most common devices utilised in manufacturing and industrial processes, used for a wide variety of automation tasks on, for example, a factory floor but the effective use of these devices requires AI to be appropriately trained. One approach to support AI training of these devices is the use of a “Digital Twin”. There are, however, a number of challenges that exist within this domain, in particular, success depends upon the ability to collect data of what are considered as observations within the environment and the application of one or many trained AI policies to the task that is to be completed. This project presents a case-study of creating and training a Robot Arm Digital Twin as an approach for AI training in a virtual space and applying this simulation learning within physical space. A virtual space, created using Unity (a contemporary Game Engine), incorporating a virtual robot arm was linked to a physical space, being a 3D printed replica of the virtual space and robot arm. These linked environments were applied to solve a task and provide training for an AI model. The contribution of this work is to provide guidance on training protocols for a digital twin together with details of the necessary architecture to support effective simulation in a virtual space through the use of Tensorflow and hyperparameter tuning. It provides an approach to addressing the mapping of learning in the virtual domain to the physical robot twin.}
}

@incollection{MOHAMMED2019187,
title = {Chapter 9 - Reinforcement learning and deep neural network for autonomous driving},
editor = {Guido Dartmann and Houbing Song and Anke Schmeink},
booktitle = {Big Data Analytics for Cyber-Physical Systems},
publisher = {Elsevier},
pages = {187-213},
year = {2019},
isbn = {978-0-12-816637-6},
doi = {https://doi.org/10.1016/B978-0-12-816637-6.00009-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128166376000099},
author = {Shawan Taha Mohammed and Andreas Bytyn and Gerd Ascheid and Guido Dartmann},
keywords = {Artificial intelligence, Autonomous driving, Reinforcement learning, Deep learning, Deep deterministic policy gradient, SUMO simulation},
abstract = {This chapter deals with a behavioral decision model for autonomous driving. Such a model aims at an application in which reinforcement learning and deep neural networks are used for autonomous driving. The training and evaluation takes place in a simulation. For this purpose, SUMO Simulation will be used to create own scenarios. An own sensor modeling will also be developed. The Deep Deterministic Policy Gradient (DDPG) method by Lillicrap et al. (2015) is used as a reinforcement learning algorithm and is adapted to this usecase. The work shows that the learned model can react to different types of drivers and the surrounding traffic. It also supports a safe and fast response to driving reactions.}
}

@article{HILLEBRAND2020266,
title = {A design methodology for deep reinforcement learning in autonomous systems},
journal = {Procedia Manufacturing},
volume = {52},
pages = {266-271},
year = {2020},
note = {System-Integrated Intelligence – Intelligent, Flexible and Connected Systems in Products and ProductionProceedings of the 5th International Conference on System-Integrated Intelligence (SysInt 2020), Bremen, Germany},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.11.044},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920321879},
author = {Michael Hillebrand and Mohsin Lakhani and Roman Dumitrescu},
keywords = {Model-Based Systems Engineering, Autonomous Systems, Deep Reinforcement Learning, Hybrid Testbed},
abstract = {Autonomous systems such as mobile robots will play an important role in fields like industrial production, transportation or in hostile environments such as space. One of the most fundamental problem in autonomous mobile robotics is autonomous navigation. It is imperative for a mobile robot to learn to navigate in complex environments such as roads or buildings. The most popular approach to this problem is to utilize different algorithms for mapping the environment, self-localization in the map, planning a trajectory to the given goal and executing this trajectory. However, there are some drawbacks of these approaches. We often make assumptions about the environment such as no dynamic or transparent objects. Moreover, there is considerable overhead, they do not learn from failures and operation scenarios. This prompts us to search for alternative approaches for autonomous navigation, such as deep reinforcement learning. However, the application of deep reinforcement learning to a particular task involves a series of non-trivial design decisions. Previous work have failed to address the need for a design methodology for deep reinforcement learning systems. In this paper, we propose design methodology and discuss relevant design decisions for deep reinforcement learning in autonomous systems. We apply the methodology to the problem of autonomous navigation.}
}

@article{SHIPMAN2019111,
title = {Reinforcement Learning and Deep Neural Networks for PI Controller Tuning},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {14},
pages = {111-116},
year = {2019},
note = {18th IFAC Symposium on Control, Optimization and Automation in Mining, Mineral and Metal Processing, MMM 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.09.173},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319308055},
author = {William J. Shipman and Loutjie C. Coetzee},
keywords = {artificial intelligence, autotuners, neural networks, SISO, proportional plus integral controllers, reinforcement learning},
abstract = {Reinforcement Learning, using deep neural networks, has recently gained prominence owing to its ability to train autonomous agents that have defeated human players in various complex games. Here, Reinforcement Learning is applied to the challenge of automatically tuning a proportional-integral controller, given only the process variable, set-point, manipulated variable and prior controller gains. The training considers random changes in plant dynamics, disturbances and measurement noise. Two training procedures were tested in this work, one that built up the difficulty of the simulation over time, and another that used the full complexity of the simulation throughout the training. The results show that building up the difficulty of the simulation by introducing greater degrees of randomness as the training progresses, produces an agent that is better able to tune the controller in question. Additional experience gathered in completing this work is also discussed to enable the reader to avoid some of the challenges encountered.}
}

@article{ZHAO2020328,
title = {Deep reinforcement learning based lane detection and localization},
journal = {Neurocomputing},
volume = {413},
pages = {328-338},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.06.094},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220310833},
author = {Zhiyuan Zhao and Qi Wang and Xuelong Li},
keywords = {Lane detection, Deep reinforcement learning, Lane localization, Q-Learning},
abstract = {Recently, deep-learning based lane detection methods effectively boost the development of Advanced Driver Assistance Systems (ADAS) and Self-Driving Systems. However, these methods only detect lane lines with sketchy bounding boxes while ignore the shape of specific curved lanes. To address the above problems, this paper introduces deep reinforcement learning into cursory lane detection models for accurate lane detection and localization. This model consists of two stages, namely the bounding box detector and landmark point localizer. To be specific, a bounding box level convolution neural network lane detector outputs the preliminary location of lanes in the form of bounding boxes. Then, a reinforcement based Deep Q-Learning Localizer (DQLL) accurately localizes the lanes as a group of landmarks to achieve better representation of curved lanes. Moreover, a pixel-level lane detection dataset named NWPU Lanes Dataset is constructed and released. It contains a variety of real traffic scenes and accurate masks of the lane lines. This approach achieves competitive performance in the released dataset and TuSimple Lane dataset. Furthermore, the codes and dataset will be released on https://github.com/tuzixini/DQLL.}
}

@article{WANG2019106,
title = {Data-driven dynamic resource scheduling for network slicing: A Deep reinforcement learning approach},
journal = {Information Sciences},
volume = {498},
pages = {106-116},
year = {2019},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2019.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0020025519303986},
author = {Haozhe Wang and Yulei Wu and Geyong Min and Jie Xu and Pengcheng Tang},
keywords = {Data-driving, End-to-End, Deep reinforcement learning, Network slicing},
abstract = {Network slicing is designed to support a variety of emerging applications with diverse performance and flexibility requirements, by dividing the physical network into multiple logical networks. These applications along with a massive number of mobile phones produce large amounts of data, bringing tremendous challenges for network slicing performance. From another perspective, this huge amount of data also offers a new opportunity for the management of network slicing resources. Leveraging the knowledge and insights retrieved from the data, we develop a novel Machine Learning-based scheme for dynamic resource scheduling for networks slicing, aiming to achieve automatic and efficient resource optimisation and End-to-End (E2E) service reliability. However, it is difficult to obtain the user-related data, which is crucial to understand the user behaviour and requests, due to the privacy issue. Therefore, Deep Reinforcement Learning (DRL) is leveraged to extract knowledge from experience by interacting with the network and enable dynamic adjustment of the resources allocated to various slices in order to maximise the resource utilisation while guaranteeing the Quality-of-Service (QoS). The experiment results demonstrate that the proposed resource scheduling scheme can dynamically allocate resources for multiple slices and meet the corresponding QoS requirements.}
}

@article{CROSS2021724,
title = {Using deep reinforcement learning to reveal how the brain encodes abstract state-space representations in high-dimensional environments},
journal = {Neuron},
volume = {109},
number = {4},
pages = {724-738.e7},
year = {2021},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2020.11.021},
url = {https://www.sciencedirect.com/science/article/pii/S0896627320308990},
author = {Logan Cross and Jeff Cockburn and Yisong Yue and John P. O’Doherty},
keywords = {fMRI, decision-making, deep reinforcement learning, naturalistic task, computational neuroscience},
abstract = {Summary
Humans possess an exceptional aptitude to efficiently make decisions from high-dimensional sensory observations. However, it is unknown how the brain compactly represents the current state of the environment to guide this process. The deep Q-network (DQN) achieves this by capturing highly nonlinear mappings from multivariate inputs to the values of potential actions. We deployed DQN as a model of brain activity and behavior in participants playing three Atari video games during fMRI. Hidden layers of DQN exhibited a striking resemblance to voxel activity in a distributed sensorimotor network, extending throughout the dorsal visual pathway into posterior parietal cortex. Neural state-space representations emerged from nonlinear transformations of the pixel space bridging perception to action and reward. These transformations reshape axes to reflect relevant high-level features and strip away information about task-irrelevant sensory features. Our findings shed light on the neural encoding of task representations for decision-making in real-world situations.}
}

@article{ABBASI202119,
title = {Deep Learning for Network Traffic Monitoring and Analysis (NTMA): A Survey},
journal = {Computer Communications},
volume = {170},
pages = {19-41},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.01.021},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421000426},
author = {Mahmoud Abbasi and Amin Shahraki and Amir Taherkordi},
keywords = {Network Traffic Monitoring and Analysis, Network management, Deep learning, Machine learning, Survey, NTMA, Edge Intelligence, IoT, QoS},
abstract = {Modern communication systems and networks, e.g., Internet of Things (IoT) and cellular networks, generate a massive and heterogeneous amount of traffic data. In such networks, the traditional network management techniques for monitoring and data analytics face some challenges and issues, e.g., accuracy, and effective processing of big data in a real-time fashion. Moreover, the pattern of network traffic, especially in cellular networks, shows very complex behavior because of various factors, such as device mobility and network heterogeneity. Deep learning has been efficiently employed to facilitate analytics and knowledge discovery in big data systems to recognize hidden and complex patterns. Motivated by these successes, researchers in the field of networking apply deep learning models for Network Traffic Monitoring and Analysis (NTMA) applications, e.g., traffic classification and prediction. This paper provides a comprehensive review on applications of deep learning in NTMA. We first provide fundamental background relevant to our review. Then, we give an insight into the confluence of deep learning and NTMA, and review deep learning techniques proposed for NTMA applications. Finally, we discuss key challenges, open issues, and future research directions for using deep learning in NTMA applications.}
}

@article{LI2021116311,
title = {Reinforcement learning based automated history matching for improved hydrocarbon production forecast},
journal = {Applied Energy},
volume = {284},
pages = {116311},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2020.116311},
url = {https://www.sciencedirect.com/science/article/pii/S0306261920316950},
author = {Hao Li and Siddharth Misra},
keywords = {Reinforcement Learning, History Matching, Reservoir Simulation, Deep Learning, Neural Network},
abstract = {History matching aims to find a numerical reservoir model that can be used to predict the reservoir performance. An engineer and model calibration (data inversion) method are required to adjust various parameters/properties of the numerical model in order to match the reservoir production history. In this study, we develop deep neural networks within the reinforcement learning framework to achieve automated history matching that will reduce engineers’ efforts, human bias, automatically and intelligently explore the parameter space, and remove the need of large set of labeled training data. To that end, a fast-marching-based reservoir simulator is encapsulated as an environment for the proposed reinforcement learning. The deep neural-network-based learning agent interacts with the reservoir simulator within reinforcement learning framework to achieve the automated history matching. Reinforcement learning techniques, such as discrete Deep Q Network and continuous Deep Deterministic Policy Gradients, are used toth, used to train the learning agents. The continuous actions enable the Deep Deterministic Policy Gradients to explore more states at each iteration in a a learning episode; consequently, a better history matching is achieved using this algorithm as compared to Deep Q Network. For simplified dual-target composite reservoir models, the best history-matching performances of the discrete and continuous learning methods in terms of normalized root mean square errors are 0.0447 and 0.0038, respectively. Our study shows that continuous action space achieved by the deep deterministic policy gradient drastically outperforms deep Q network.}
}

@article{LI202098,
title = {An effective maximum entropy exploration approach for deceptive game in reinforcement learning},
journal = {Neurocomputing},
volume = {403},
pages = {98-108},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.04.068},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220306536},
author = {Chunmao Li and Xuanguang Wei and Yinliang Zhao and Xupeng Geng},
keywords = {Deep reinforcement learning, Deceptive game, Maximum entropy explorer approach, Experience replay, On-policy switch},
abstract = {Deceptive games are games that utilize the reward structure to keep the agent away from the global optimization and have been grown up to become a huge challenge in the field of deep reinforcement learning intelligent exploration. Most of the cutting-edge exploration approaches, such as count-based and curiosity-driven, even with intrinsic motivation, which achieves better performance in the sparse reward game, still easily fall into local optimal traps in the deceptive game. To address this shortfall, we introduce a further exploration approach called Maximum Entropy Explore (MEE). Based on entropy rewards and the off-policy actor-critic reinforcement learning algorithm, we divided the agent exploration policy into two independent parts, namely, the target policy and the explorer policy. The explorer policy, taking the maximum entropy of the target policy as the optimization goal, is used to interact with the environment and generated trajectories for the target policy. The target policy regards the maximization of external reward as the optimization goal to achieve the global solution. To alleviate the catastrophic forgetting problem which leads to the training of the agent not stabilized during the off-policy exploration phrase, the optimal experience replay is applied. An on-policy mode switch trick is used to validly prevent the unstable and diverge which caused by the deadly triad. We conduct experiments comparing our approach with state-of-the-art deep reinforcement learning algorithm and exploration methods in the grid world and StarCraft II environments with deceptive reward. The experiment indicates that the MME approach sets out to be in the present paper effectively avoids the deceptive reward trap and learns the global optimal strategy.}
}

