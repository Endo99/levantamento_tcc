Wellington Joji Endo

@article{10.1145/3358230,
author = {Tran, Hoang-Dung and Cai, Feiyang and Diego, Manzanas Lopez and Musau, Patrick and Johnson, Taylor T. and Koutsoukos, Xenofon},
title = {Safety Verification of Cyber-Physical Systems with Reinforcement Learning Control},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {5s},
issn = {1539-9087},
url = {https://doi.org/10.1145/3358230},
doi = {10.1145/3358230},
abstract = {This paper proposes a new forward reachability analysis approach to verify safety of cyber-physical systems (CPS) with reinforcement learning controllers. The foundation of our approach lies on two efficient, exact and over-approximate reachability algorithms for neural network control systems using star sets, which is an efficient representation of polyhedra. Using these algorithms, we determine the initial conditions for which a safety-critical system with a neural network controller is safe by incrementally searching a critical initial condition where the safety of the system cannot be established. Our approach produces tight over-approximation error and it is computationally efficient, which allows the application to practical CPS with learning enable components (LECs). We implement our approach in NNV, a recent verification tool for neural networks and neural network control systems, and evaluate its advantages and applicability by verifying safety of a practical Advanced Emergency Braking System (AEBS) with a reinforcement learning (RL) controller trained using the deep deterministic policy gradient (DDPG) method. The experimental results show that our new reachability algorithms are much less conservative than existing polyhedra-based approaches. We successfully determine the entire region of the initial conditions of the AEBS with the RL controller such that the safety of the system is guaranteed, while a polyhedra-based approach cannot prove the safety properties of the system.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = oct,
articleno = {105},
numpages = {22},
keywords = {reinforcement learning, verification, Formal methods}
}

@article{10.5555/3291125.3291134,
author = {De Bruin, Tim and Kober, Jens and Tuyls, Karl and Babu\v{s}ka, Robert},
title = {Experience Selection in Deep Reinforcement Learning for Control},
year = {2018},
issue_date = {January 2018},
publisher = {JMLR.org},
volume = {19},
number = {1},
issn = {1532-4435},
abstract = {Experience replay is a technique that allows off-policy reinforcement-learning methods to reuse past experiences. The stability and speed of convergence of reinforcement learning, as well as the eventual performance of the learned policy, are strongly dependent on the experiences being replayed. Which experiences are replayed depends on two important choices. The first is which and how many experiences to retain in the experience replay buffer. The second choice is how to sample the experiences that are to be replayed from that buffer. We propose new methods for the combined problem of experience retention and experience sampling. We refer to the combination as experience selection. We focus our investigation specifically on the control of physical systems, such as robots, where exploration is costly. To determine which experiences to keep and which to replay, we investigate different proxies for their immediate and long-term utility. These proxies include age, temporal difference error and the strength of the applied exploration noise. Since no currently available method works in all situations, we propose guidelines for using prior knowledge about the characteristics of the control problem at hand to choose the appropriate experience replay strategy.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {347–402},
numpages = {56},
keywords = {robotics, experience replay, control, reinforcement learning, deep learning}
}

@article{10.5555/3122009.3208017,
author = {Wirth, Christian and Akrour, Riad and Neumann, Gerhard and F\"{u}rnkranz, Johannes},
title = {A Survey of Preference-Based Reinforcement Learning Methods},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {Reinforcement learning (RL) techniques optimize the accumulated long-term reward of a suitably chosen reward function. However, designing such a reward function often requires a lot of task-specific prior knowledge. The designer needs to consider different objectives that do not only influence the learned behavior but also the learning progress. To alleviate these issues, preference-based reinforcement learning algorithms (PbRL) have been proposed that can directly learn from an expert's preferences instead of a hand-designed numeric reward. PbRL has gained traction in recent years due to its ability to resolve the reward shaping problem, its ability to learn from non numeric rewards and the possibility to reduce the dependence on expert knowledge. We provide a unified framework for PbRL that describes the task formally and points out the different design principles that affect the evaluation task for the human as well as the computational complexity. The design principles include the type of feedback that is assumed, the representation that is learned to capture the preferences, the optimization problem that has to be solved as well as how the exploration/exploitation problem is tackled. Furthermore, we point out shortcomings of current algorithms, propose open research questions and briefly survey practical tasks that have been solved using PbRL.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {4945–4990},
numpages = {46},
keywords = {policy search, Markov decision process, reinforcement learning, temporal difference learning, qualitative feedback, preference-based reinforcement learning, preference learning}
}

@INPROCEEDINGS{8785463,
author={R. {Tan} and J. {Zhou} and H. {Du} and S. {Shang} and L. {Dai}},
booktitle={2019 IEEE 8th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)},
title={An modeling processing method for video games based on deep reinforcement learning},
year={2019},
volume={},
number={},
pages={939-942},
doi={10.1109/ITAIC.2019.8785463}}

@INPROCEEDINGS{8666545,
author={A. {Jeerige} and D. {Bein} and A. {Verma}},
booktitle={2019 IEEE 9th Annual Computing and Communication Workshop and Conference (CCWC)},
title={Comparison of Deep Reinforcement Learning Approaches for Intelligent Game Playing},
year={2019},
volume={},
number={},
pages={0366-0371},
doi={10.1109/CCWC.2019.8666545}}

@article{10.1145/3285029,
author = {Zhang, Shuai and Yao, Lina and Sun, Aixin and Tay, Yi},
title = {Deep Learning Based Recommender System: A Survey and New Perspectives},
year = {2019},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3285029},
doi = {10.1145/3285029},
abstract = {With the growing volume of online information, recommender systems have been an effective strategy to overcome information overload. The utility of recommender systems cannot be overstated, given their widespread adoption in many web applications, along with their potential impact to ameliorate many problems related to over-choice. In recent years, deep learning has garnered considerable interest in many research fields such as computer vision and natural language processing, owing not only to stellar performance but also to the attractive property of learning feature representations from scratch. The influence of deep learning is also pervasive, recently demonstrating its effectiveness when applied to information retrieval and recommender systems research. The field of deep learning in recommender system is flourishing. This article aims to provide a comprehensive review of recent research efforts on deep learning-based recommender systems. More concretely, we provide and devise a taxonomy of deep learning-based recommendation models, along with a comprehensive summary of the state of the art. Finally, we expand on current trends and provide new perspectives pertaining to this new and exciting development of the field.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {5},
numpages = {38},
keywords = {deep learning, survey, Recommender system}
}

@ARTICLE{8277160,
author={M. {Mahmud} and M. S. {Kaiser} and A. {Hussain} and S. {Vassanelli}},
journal={IEEE Transactions on Neural Networks and Learning Systems},
title={Applications of Deep Learning and Reinforcement Learning to Biological Data},
year={2018},
volume={29},
number={6},
pages={2063-2079},
doi={10.1109/TNNLS.2018.2790388}}

@ARTICLE{9314886,
author={I. {Oh} and S. {Rho} and S. {Moon} and S. {Son} and H. {Lee} and J. {Chung}},
journal={IEEE Transactions on Games},
title={Creating Pro-Level AI for a Real-Time Fighting Game Using Deep Reinforcement Learning},
year={2021},
volume={},
number={},
pages={1-1},
doi={10.1109/TG.2021.3049539}}

@INPROCEEDINGS{8729310,
author={M. {Wang} and L. {Wang} and T. {Yue}},
booktitle={2019 IEEE 3rd Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)},
title={An Application of Continuous Deep Reinforcement Learning Approach to Pursuit-Evasion Differential Game},
year={2019},
volume={},
number={},
pages={1150-1156},
doi={10.1109/ITNEC.2019.8729310}}

@INPROCEEDINGS{9036763,
author={F. {Moreno-Vera}},
booktitle={2019 IEEE Latin American Conference on Computational Intelligence (LA-CCI)},
title={Performing Deep Recurrent Double Q-Learning for Atari Games},
year={2019},
volume={},
number={},
pages={1-4},
doi={10.1109/LA-CCI47412.2019.9036763}}

@inproceedings{10.1145/3328526.3329634,
author = {Wright, Mason and Wang, Yongzhao and Wellman, Michael P.},
title = {Iterated Deep Reinforcement Learning in Games: History-Aware Training for Improved Stability},
year = {2019},
isbn = {9781450367929},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328526.3329634},
doi = {10.1145/3328526.3329634},
abstract = {Deep reinforcement learning (RL) is a powerful method for generating policies in complex environments, and recent breakthroughs in game-playing have leveraged deep RL as part of an iterative multiagent search process. We build on such developments and present an approach that learns progressively better mixed strategies in complex dynamic games of imperfect information, through iterated use of empirical game-theoretic analysis (EGTA) with deep RL policies. We apply the approach to a challenging cybersecurity game defined over attack graphs. Iterating deep RL with EGTA to convergence over dozens of rounds, we generate mixed strategies far stronger than earlier published heuristic strategies for this game. We further refine the strategy-exploration process, by fine-tuning in a training environment that includes out-of-equilibrium but recently seen opponents. Experiments suggest this history-aware approach yields strategies with lower regret at each stage of training.},
booktitle = {Proceedings of the 2019 ACM Conference on Economics and Computation},
pages = {617–636},
numpages = {20},
keywords = {double oracle, deep reinforcement learning, multi-agent reinforcement learning, attack graphs, security games},
location = {Phoenix, AZ, USA},
series = {EC '19}
}

@inproceedings{10.1145/3278721.3278776,
author = {Iyer, Rahul and Li, Yuezhang and Li, Huao and Lewis, Michael and Sundar, Ramitha and Sycara, Katia},
title = {Transparency and Explanation in Deep Reinforcement Learning Neural Networks},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278776},
doi = {10.1145/3278721.3278776},
abstract = {Autonomous AI systems will be entering human society in the near future to provide services and work alongside humans. For those systems to be accepted and trusted, the users should be able to understand the reasoning process of the system, i.e. the system should be transparent. System transparency enables humans to form coherent explanations of the system's decisions and actions. Transparency is important not only for user trust, but also for software debugging and certification. In recent years, Deep Neural Networks have made great advances in multiple application areas. However, deep neural networks are opaque. In this paper, we report on work in transparency in Deep Reinforcement Learning Networks (DRLN). Such networks have been extremely successful in accurately learning action control in image input domains, such as Atari games. In this paper, we propose a novel and general method that (a) incorporates explicit object recognition processing into deep reinforcement learning models, (b) forms the basis for the development of "object saliency maps", to provide visualization of internal states of DRLNs, thus enabling the formation of explanations and (c) can be incorporated in any existing deep reinforcement learning framework. We present computational results and human experiments to evaluate our approach.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {144–150},
numpages = {7},
keywords = {explainable ai, deep reinforcement learning, system transparency, human factors, human-ai interaction},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3377929.3398093,
author = {Waris, Faisal and Reynolds, Robert},
title = {Neuro-Evolution Using Game-Driven Cultural Algorithms},
year = {2020},
isbn = {9781450371278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377929.3398093},
doi = {10.1145/3377929.3398093},
abstract = {Contemporary 'deep learning' (DL) models have proven to be effective in a wide variety of applications. However, the right network topology for the problem at hand may be complex and not immediately obvious. This has given rise to the secondary field of neural architecture search (NAS). This paper describes a NAS method based on graph evolution pioneered by Neuro-evolution of Augmenting Topologies (NEAT), but driven by the evolutionary mechanisms underlying Cultural Algorithms (CA). CA is a population-based, stochastic optimization system inspired by problem solving in human cultures, suited to solving problems such as NAS. We present CATNeuro a system for evolving DL models guided by CA metaheuristics called Knowledge Sources (KS). The KS store knowledge harvested from prior generations and use it to guide subsequent generations in the search space. A knowledge distribution mechanism, which assigns a KS to each individual in the population, is an instrumental part of this process. CATNeuro, is applied to find optimal network topologies to play a 2D fighting game called FightingICE (based on "The Rumble Fish" game). A policy-based, reinforcement learning method is used to create the training data for network optimization. CATNeuro is still evolving. In this primary foray into NAS, we contrast the performance of CATNeuro with two different knowledge distribution mechanisms - the stalwart Weighted Majority (WTD) - which represents "wisdom of the crowds" - and a new one based on the Stag-Hunt game from evolutionary game theory. We show that Stag-Hunt has a statistical edge over WTD in many areas and thus is a better candidate for future development with CATNeuro.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion},
pages = {1857–1865},
numpages = {9},
keywords = {ACM proceedings, neural architecture search, game theory, cultural algorithms, reinforcement learning},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@inproceedings{10.1145/3029798.3038399,
author = {Shih, Victor and Jangraw, David and Saproo, Sameer and Sajda, Paul},
title = {Deep Reinforcement Learning Using Neurophysiological Signatures of Interest},
year = {2017},
isbn = {9781450348850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3029798.3038399},
doi = {10.1145/3029798.3038399},
abstract = {We present a study where human neurophysiological signals are used as implicit feedback to alter the behavior of a deep learning based autonomous driving agent in a simulated virtual environment.},
booktitle = {Proceedings of the Companion of the 2017 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {285–286},
numpages = {2},
keywords = {bci, deep reinforcement learning, autonomous vehicles, affective computing},
location = {Vienna, Austria},
series = {HRI '17}
}

@INPROCEEDINGS{9070827,
author={A. J. {Almalki} and P. {Wocjan}},
booktitle={2019 International Conference on Computational Science and Computational Intelligence (CSCI)},
title={Exploration of Reinforcement Learning to Play Snake Game},
year={2019},
volume={},
number={},
pages={377-381},
doi={10.1109/CSCI49370.2019.00073}}

@INPROCEEDINGS{8490409,
author={P. {Andersen} and M. {Goodwin} and O. {Granmo}},
booktitle={2018 IEEE Conference on Computational Intelligence and Games (CIG)},
title={Deep RTS: A Game Environment for Deep Reinforcement Learning in Real-Time Strategy Games},
year={2018},
volume={},
number={},
pages={1-8},
doi={10.1109/CIG.2018.8490409}}

@INPROCEEDINGS{8320266,
author={H. {Singal} and P. {Aggarwal} and V. {Dutt}},
booktitle={2017 International Conference on Machine Learning and Data Science (MLDS)},
title={Modeling Decisions in Games Using Reinforcement Learning},
year={2017},
volume={},
number={},
pages={98-105},
doi={10.1109/MLDS.2017.13}}

@INPROCEEDINGS{8636940,
author={P. B. S. {Serafim} and Y. L. B. {Nogueira} and C. A. {Vidal} and J. B. C. {Neto}},
booktitle={2018 17th Brazilian Symposium on Computer Games and Digital Entertainment (SBGames)},
title={Evaluating Competition in Training of Deep Reinforcement Learning Agents in First-Person Shooter Games},
year={2018},
volume={},
number={},
pages={117-11709},
doi={10.1109/SBGAMES.2018.00023}}

@INPROCEEDINGS{9277188,
author={J. {Hu} and F. {Zhao} and J. {Meng} and S. {Wu}},
booktitle={2020 IEEE International Conference on Information Technology,Big Data and Artificial Intelligence (ICIBA)},
title={Application of Deep Reinforcement Learning in the Board Game},
year={2020},
volume={1},
number={},
pages={809-812},
doi={10.1109/ICIBA50161.2020.9277188}}

@INPROCEEDINGS{8400328,
author={P. B. S. {Serafim} and Y. L. B. {Nogueira} and C. {Vidal} and J. {Cavalcante-Neto}},
booktitle={2017 16th Brazilian Symposium on Computer Games and Digital Entertainment (SBGames)},
title={On the Development of an Autonomous Agent for a 3D First-Person Shooter Game Using Deep Reinforcement Learning},
year={2017},
volume={},
number={},
pages={155-163},
doi={10.1109/SBGames.2017.00025}}

@INPROCEEDINGS{8080451,
author={S. {Yoon} and K. {Kim}},
booktitle={2017 IEEE Conference on Computational Intelligence and Games (CIG)},
title={Deep Q networks for visual fighting game AI},
year={2017},
volume={},
number={},
pages={306-308},
doi={10.1109/CIG.2017.8080451}}

@INPROCEEDINGS{8460004,
author={Z. {Wei} and D. {Wang} and M. {Zhang} and A. {Tan} and C. {Miao} and Y. {Zhou}},
booktitle={2018 IEEE International Conference on Agents (ICA)},
title={Autonomous Agents in Snake Game via Deep Reinforcement Learning},
year={2018},
volume={},
number={},
pages={20-25},
doi={10.1109/AGENTS.2018.8460004}}

@INPROCEEDINGS{8588472,
author={T. {Wang} and T. {Kaneko}},
booktitle={2018 Conference on Technologies and Applications of Artificial Intelligence (TAAI)},
title={Application of Deep Reinforcement Learning in Werewolf Game Agents},
year={2018},
volume={},
number={},
pages={28-33},
doi={10.1109/TAAI.2018.00016}}

@INPROCEEDINGS{8578287,
author={K. M. {Lee} and H. {Myeong} and G. {Song}},
booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
title={SeedNet: Automatic Seed Generation with Deep Reinforcement Learning for Robust Interactive Segmentation},
year={2018},
volume={},
number={},
pages={1760-1768},
doi={10.1109/CVPR.2018.00189}}

@ARTICLE{9106862,
author={Y. {Liang} and C. {Guo} and Z. {Ding} and H. {Hua}},
journal={IEEE Transactions on Power Systems},
title={Agent-Based Modeling in Electricity Market Using Deep Deterministic Policy Gradient Algorithm},
year={2020},
volume={35},
number={6},
pages={4180-4192},
doi={10.1109/TPWRS.2020.2999536}}

@INPROCEEDINGS{9313387,
author={H. {Wang} and H. {Tang} and J. {Hao} and X. {Hao} and Y. {Fu} and Y. {Ma}},
booktitle={2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)},
title={Large Scale Deep Reinforcement Learning in War-games},
year={2020},
volume={},
number={},
pages={1693-1699},
doi={10.1109/BIBM49941.2020.9313387}}

@INPROCEEDINGS{9206918,
author={R. {Yang} and H. {Tang} and B. {Jin}},
booktitle={2020 International Joint Conference on Neural Networks (IJCNN)},
title={Monoceros: A New Approach for Training an Agent to Play FPS Games},
year={2020},
volume={},
number={},
pages={1-8},
doi={10.1109/IJCNN48605.2020.9206918}}

@inproceedings{10.1145/3318265.3318294,
author = {Zhou, Yinda and Liu, Weiming and Li, Bin},
title = {Two-Stage Population Based Training Method for Deep Reinforcement Learning},
year = {2019},
isbn = {9781450366380},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318265.3318294},
doi = {10.1145/3318265.3318294},
abstract = {Deep reinforcement learning (DRL) methods has been widely applied on more and more challenging learning tasks, and achieved excellent performance. However, the efficiency of deep reinforcement learning is notoriously sensitive to their own hyperparameter configuration. The optimization process of deep reinforcement learning is highly dynamic and non-stationary, rather than a simple fitting process. So, its optimal hyperparameter should be adaptively adjusted according to the current learning process, rather than using a fixed set of hyperparameter configurations from beginning to end. DeepMind innovatively proposed a population based training (PBT) method for deep reinforcement learning, which achieved hyperparameter adaptation and made the model better trained. However, we assume that at the early stage when the learning model has little knowledge of the environment, frequent hyperparameter change will not be helpful for the model to learn efficiently, while learning with a reasonable fixed hyperparameter configuration will help the model obtain necessary knowledge as quick as possible, which we consider is more important for reinforcement learning at early stage. In this paper, we verified our hypothesis through experiments, and a Two-Stage Population Based Training (TS-PBT) method is proposed, which is a more efficient population based training method for deep reinforcement learning. Experiments show that at the same computational budget, our TS-PBT method makes the final performance of the model significantly better than the PBT method. TS-PBT achieved 40%, 310%, 2%, 53%, 30% and 38% performance improvement over PBT separately in six test environments.},
booktitle = {Proceedings of the 3rd International Conference on High Performance Compilation, Computing and Communications},
pages = {38–44},
numpages = {7},
keywords = {deep reinforcement learning, PBT, two-stage, hyperparameter adaptation, game},
location = {Xi'an, China},
series = {HP3C '19}
}

@inproceedings{10.5555/3306127.3331943,
author = {Carr, Thomas and Chli, Maria and Vogiatzis, George},
title = {Domain Adaptation for Reinforcement Learning on the Atari},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Deep Reinforcement learning is a powerful machine learning paradigm that has had significant success across a wide range of control problems. This success often requires long training times to achieve. Observing that many problems share similarities, it is likely that much of the training done could be redundant if knowledge could be efficiently and appropriately shared across tasks. In this paper we demonstrate a novel adversarial domain adaptation approach to transfer state knowledge between domains and tasks on the Atari game suite. We show how this approach can successfully transfer across very different visual domains of the Atari platform. We focus on semantically related games that involve returning a ball with the user controlled agent. Our experiments demonstrate that our method reduces the number of samples required to successfully train an agent to play an Atari game.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1859–1861},
numpages = {3},
keywords = {reinforcement learning, domain adapatation, deep learning},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@article{ATKINSON2021291,
title = {Pseudo-rehearsal: Achieving deep reinforcement learning without catastrophic forgetting},
journal = {Neurocomputing},
volume = {428},
pages = {291-307},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.11.050},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220318439},
author = {Craig Atkinson and Brendan McCane and Lech Szymanski and Anthony Robins},
keywords = {Deep reinforcement learning, Pseudo-rehearsal, Catastrophic forgetting, Generative adversarial network},
abstract = {Neural networks can achieve excellent results in a wide variety of applications. However, when they attempt to sequentially learn, they tend to learn the new task while catastrophically forgetting previous ones. We propose a model that overcomes catastrophic forgetting in sequential reinforcement learning by combining ideas from continual learning in both the image classification domain and the reinforcement learning domain. This model features a dual memory system which separates continual learning from reinforcement learning and a pseudo-rehearsal system that “recalls” items representative of previous tasks via a deep generative network. Our model sequentially learns Atari 2600 games without demonstrating catastrophic forgetting and continues to perform above human level on all three games. This result is achieved without: demanding additional storage requirements as the number of tasks increases, storing raw data or revisiting past tasks. In comparison, previous state-of-the-art solutions are substantially more vulnerable to forgetting on these complex deep reinforcement learning tasks.}
}

@article{DING2020102069,
title = {A deep reinforcement learning for user association and power control in heterogeneous networks},
journal = {Ad Hoc Networks},
volume = {102},
pages = {102069},
year = {2020},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2019.102069},
url = {https://www.sciencedirect.com/science/article/pii/S1570870519310546},
author = {Hui Ding and Feng Zhao and Jie Tian and Dongyang Li and Haixia Zhang},
keywords = {Heterogeneous networks, User association, Power control, Reinforcement learning, Deep Q-learning network},
abstract = {Heterogeneous network (HetNet) is a promising solution to satisfy the unprecedented demand for higher data rate in the next generation mobile networks. Different from the traditional single-layer cellular networks, how to provide the best service to the user equipments (UEs) under the limited resource is an urgent problem to solve. In order to efficiently address the above challenge and strive towards high network energy efficiency, the joint optimization problem of user association and power control in orthogonal frequency division multiple access (OFDMA) based uplink HetNets is studied. Considering the non-convex and non-linear characteristics of the problem, a multi-agent deep Q-learning Network (DQN) method is studied to solve the problem. Different from the traditional methods, such as game theory, fractional programming and convex optimization, which need more and accurate network information in practice, the multi-agent DQN method requires less communication information of the environment. Moreover, for the communication environment dynamics, the maximum long-term overall network utility with a new reward function while ensuring the UE’s quality of service (QoS) requirements is achieved by using the multi-agent DQN method. Then, according to the application scenario, the action space, state space and reward function of the multi-agent DQN based framework are redefined and formulated. Simulation results demonstrate that the multi-agent DQN method has the best performance on convergence and energy efficiency compared with the traditional reinforcement learning (Q-learning).}
}

@article{LI2020157,
title = {Accelerating deep reinforcement learning model for game strategy},
journal = {Neurocomputing},
volume = {408},
pages = {157-168},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.06.110},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220303337},
author = {Yifan Li and Yuchun Fang and Zahid Akhtar},
keywords = {Deep reinforcement learning, Convolutional neural network, Depthwise separable convolution, Binary weight network},
abstract = {In recent years, deep reinforcement learning has achieved impressing accuracies in games compared with traditional methods. Prior schemes utilized Convolutional Neural Networks (CNNs) or Long Short-Term Memory networks (LSTMs) to improve the performances of the agents. In this paper, we consider the issue from a different perspective when the training and inference of deep reinforcement learning are required to be performed with limited computing resources. Mainly, we propose two efficient neural network architectures of deep reinforcement learning: Light-Q-Network (LQN) and Binary-Q-Network (BQN). In LQN, The depth-wise separable CNNs are utilized in memory and computation saving. While, in BQN, the weights of convolutional layers are binary that help in shortening the training time and reduce memory consumption. We evaluate our approach on Atari 2600 domain and StarCraft II mini-games. The results demonstratethe efficiency of the proposed architectures. Though performances of agents in most games are still super-human, the proposed methods advance the agent from sub to super-human performance in particular games. Also, we empirically find that non-standard convolution and non-full-precision networks do not affect agent learning game strategy.}
}

@article{BU2019500,
title = {A smart agriculture IoT system based on deep reinforcement learning},
journal = {Future Generation Computer Systems},
volume = {99},
pages = {500-507},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.04.041},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19307277},
author = {Fanyu Bu and Xin Wang},
keywords = {Deep reinforcement learning, Smart agriculture IoT, Edge computing, Cloud computing},
abstract = {Smart agriculture systems based on Internet of Things are the most promising to increase food production and reduce the consumption of resources like fresh water. In this study, we present a smart agriculture IoT system based on deep reinforcement learning which includes four layers, namely agricultural data collection layer, edge computing layer, agricultural data transmission layer, and cloud computing layer. The presented system integrates some advanced information techniques, especially artificial intelligence and cloud computing, with agricultural production to increase food production. Specially, the most advanced artificial intelligence model, deep reinforcement learning is combined in the cloud layer to make immediate smart decisions such as determining the amount of water needed to be irrigated for improving crop growth environment. We present several representative deep reinforcement learning models with their broad applications. Finally, we talk about the open challenges and the potential applications of deep reinforcement learning in smart agriculture IoT systems.}
}

@article{MATULIS2021,
title = {A robot arm digital twin utilising reinforcement learning},
journal = {Computers & Graphics},
year = {2021},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2021.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S009784932100011X},
author = {Marius Matulis and Carlo Harvey},
keywords = {Robot arm, Reinforcement learning, Artificial intelligence, Digital twin},
abstract = {For many industry contexts, the implementation of Artificial Intelligence (AI) has contributed to what has become known as the fourth industrial revolution or “Industry 4.0” and creates an opportunity to deliver significant benefit to both businesses and their stakeholders. Robot arms are one of the most common devices utilised in manufacturing and industrial processes, used for a wide variety of automation tasks on, for example, a factory floor but the effective use of these devices requires AI to be appropriately trained. One approach to support AI training of these devices is the use of a “Digital Twin”. There are, however, a number of challenges that exist within this domain, in particular, success depends upon the ability to collect data of what are considered as observations within the environment and the application of one or many trained AI policies to the task that is to be completed. This project presents a case-study of creating and training a Robot Arm Digital Twin as an approach for AI training in a virtual space and applying this simulation learning within physical space. A virtual space, created using Unity (a contemporary Game Engine), incorporating a virtual robot arm was linked to a physical space, being a 3D printed replica of the virtual space and robot arm. These linked environments were applied to solve a task and provide training for an AI model. The contribution of this work is to provide guidance on training protocols for a digital twin together with details of the necessary architecture to support effective simulation in a virtual space through the use of Tensorflow and hyperparameter tuning. It provides an approach to addressing the mapping of learning in the virtual domain to the physical robot twin.}
}

@incollection{MOHAMMED2019187,
title = {Chapter 9 - Reinforcement learning and deep neural network for autonomous driving},
editor = {Guido Dartmann and Houbing Song and Anke Schmeink},
booktitle = {Big Data Analytics for Cyber-Physical Systems},
publisher = {Elsevier},
pages = {187-213},
year = {2019},
isbn = {978-0-12-816637-6},
doi = {https://doi.org/10.1016/B978-0-12-816637-6.00009-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128166376000099},
author = {Shawan Taha Mohammed and Andreas Bytyn and Gerd Ascheid and Guido Dartmann},
keywords = {Artificial intelligence, Autonomous driving, Reinforcement learning, Deep learning, Deep deterministic policy gradient, SUMO simulation},
abstract = {This chapter deals with a behavioral decision model for autonomous driving. Such a model aims at an application in which reinforcement learning and deep neural networks are used for autonomous driving. The training and evaluation takes place in a simulation. For this purpose, SUMO Simulation will be used to create own scenarios. An own sensor modeling will also be developed. The Deep Deterministic Policy Gradient (DDPG) method by Lillicrap et al. (2015) is used as a reinforcement learning algorithm and is adapted to this usecase. The work shows that the learned model can react to different types of drivers and the surrounding traffic. It also supports a safe and fast response to driving reactions.}
}

@article{SHIPMAN2019111,
title = {Reinforcement Learning and Deep Neural Networks for PI Controller Tuning},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {14},
pages = {111-116},
year = {2019},
note = {18th IFAC Symposium on Control, Optimization and Automation in Mining, Mineral and Metal Processing, MMM 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.09.173},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319308055},
author = {William J. Shipman and Loutjie C. Coetzee},
keywords = {artificial intelligence, autotuners, neural networks, SISO, proportional plus integral controllers, reinforcement learning},
abstract = {Reinforcement Learning, using deep neural networks, has recently gained prominence owing to its ability to train autonomous agents that have defeated human players in various complex games. Here, Reinforcement Learning is applied to the challenge of automatically tuning a proportional-integral controller, given only the process variable, set-point, manipulated variable and prior controller gains. The training considers random changes in plant dynamics, disturbances and measurement noise. Two training procedures were tested in this work, one that built up the difficulty of the simulation over time, and another that used the full complexity of the simulation throughout the training. The results show that building up the difficulty of the simulation by introducing greater degrees of randomness as the training progresses, produces an agent that is better able to tune the controller in question. Additional experience gathered in completing this work is also discussed to enable the reader to avoid some of the challenges encountered.}
}

@article{ZHAO2020328,
title = {Deep reinforcement learning based lane detection and localization},
journal = {Neurocomputing},
volume = {413},
pages = {328-338},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.06.094},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220310833},
author = {Zhiyuan Zhao and Qi Wang and Xuelong Li},
keywords = {Lane detection, Deep reinforcement learning, Lane localization, Q-Learning},
abstract = {Recently, deep-learning based lane detection methods effectively boost the development of Advanced Driver Assistance Systems (ADAS) and Self-Driving Systems. However, these methods only detect lane lines with sketchy bounding boxes while ignore the shape of specific curved lanes. To address the above problems, this paper introduces deep reinforcement learning into cursory lane detection models for accurate lane detection and localization. This model consists of two stages, namely the bounding box detector and landmark point localizer. To be specific, a bounding box level convolution neural network lane detector outputs the preliminary location of lanes in the form of bounding boxes. Then, a reinforcement based Deep Q-Learning Localizer (DQLL) accurately localizes the lanes as a group of landmarks to achieve better representation of curved lanes. Moreover, a pixel-level lane detection dataset named NWPU Lanes Dataset is constructed and released. It contains a variety of real traffic scenes and accurate masks of the lane lines. This approach achieves competitive performance in the released dataset and TuSimple Lane dataset. Furthermore, the codes and dataset will be released on https://github.com/tuzixini/DQLL.}
}

@article{WANG2019106,
title = {Data-driven dynamic resource scheduling for network slicing: A Deep reinforcement learning approach},
journal = {Information Sciences},
volume = {498},
pages = {106-116},
year = {2019},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2019.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0020025519303986},
author = {Haozhe Wang and Yulei Wu and Geyong Min and Jie Xu and Pengcheng Tang},
keywords = {Data-driving, End-to-End, Deep reinforcement learning, Network slicing},
abstract = {Network slicing is designed to support a variety of emerging applications with diverse performance and flexibility requirements, by dividing the physical network into multiple logical networks. These applications along with a massive number of mobile phones produce large amounts of data, bringing tremendous challenges for network slicing performance. From another perspective, this huge amount of data also offers a new opportunity for the management of network slicing resources. Leveraging the knowledge and insights retrieved from the data, we develop a novel Machine Learning-based scheme for dynamic resource scheduling for networks slicing, aiming to achieve automatic and efficient resource optimisation and End-to-End (E2E) service reliability. However, it is difficult to obtain the user-related data, which is crucial to understand the user behaviour and requests, due to the privacy issue. Therefore, Deep Reinforcement Learning (DRL) is leveraged to extract knowledge from experience by interacting with the network and enable dynamic adjustment of the resources allocated to various slices in order to maximise the resource utilisation while guaranteeing the Quality-of-Service (QoS). The experiment results demonstrate that the proposed resource scheduling scheme can dynamically allocate resources for multiple slices and meet the corresponding QoS requirements.}
}

@article{CROSS2021724,
title = {Using deep reinforcement learning to reveal how the brain encodes abstract state-space representations in high-dimensional environments},
journal = {Neuron},
volume = {109},
number = {4},
pages = {724-738.e7},
year = {2021},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2020.11.021},
url = {https://www.sciencedirect.com/science/article/pii/S0896627320308990},
author = {Logan Cross and Jeff Cockburn and Yisong Yue and John P. O’Doherty},
keywords = {fMRI, decision-making, deep reinforcement learning, naturalistic task, computational neuroscience},
abstract = {Summary
Humans possess an exceptional aptitude to efficiently make decisions from high-dimensional sensory observations. However, it is unknown how the brain compactly represents the current state of the environment to guide this process. The deep Q-network (DQN) achieves this by capturing highly nonlinear mappings from multivariate inputs to the values of potential actions. We deployed DQN as a model of brain activity and behavior in participants playing three Atari video games during fMRI. Hidden layers of DQN exhibited a striking resemblance to voxel activity in a distributed sensorimotor network, extending throughout the dorsal visual pathway into posterior parietal cortex. Neural state-space representations emerged from nonlinear transformations of the pixel space bridging perception to action and reward. These transformations reshape axes to reflect relevant high-level features and strip away information about task-irrelevant sensory features. Our findings shed light on the neural encoding of task representations for decision-making in real-world situations.}
}

@article{ABBASI202119,
title = {Deep Learning for Network Traffic Monitoring and Analysis (NTMA): A Survey},
journal = {Computer Communications},
volume = {170},
pages = {19-41},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.01.021},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421000426},
author = {Mahmoud Abbasi and Amin Shahraki and Amir Taherkordi},
keywords = {Network Traffic Monitoring and Analysis, Network management, Deep learning, Machine learning, Survey, NTMA, Edge Intelligence, IoT, QoS},
abstract = {Modern communication systems and networks, e.g., Internet of Things (IoT) and cellular networks, generate a massive and heterogeneous amount of traffic data. In such networks, the traditional network management techniques for monitoring and data analytics face some challenges and issues, e.g., accuracy, and effective processing of big data in a real-time fashion. Moreover, the pattern of network traffic, especially in cellular networks, shows very complex behavior because of various factors, such as device mobility and network heterogeneity. Deep learning has been efficiently employed to facilitate analytics and knowledge discovery in big data systems to recognize hidden and complex patterns. Motivated by these successes, researchers in the field of networking apply deep learning models for Network Traffic Monitoring and Analysis (NTMA) applications, e.g., traffic classification and prediction. This paper provides a comprehensive review on applications of deep learning in NTMA. We first provide fundamental background relevant to our review. Then, we give an insight into the confluence of deep learning and NTMA, and review deep learning techniques proposed for NTMA applications. Finally, we discuss key challenges, open issues, and future research directions for using deep learning in NTMA applications.}
}

@article{LI2021116311,
title = {Reinforcement learning based automated history matching for improved hydrocarbon production forecast},
journal = {Applied Energy},
volume = {284},
pages = {116311},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2020.116311},
url = {https://www.sciencedirect.com/science/article/pii/S0306261920316950},
author = {Hao Li and Siddharth Misra},
keywords = {Reinforcement Learning, History Matching, Reservoir Simulation, Deep Learning, Neural Network},
abstract = {History matching aims to find a numerical reservoir model that can be used to predict the reservoir performance. An engineer and model calibration (data inversion) method are required to adjust various parameters/properties of the numerical model in order to match the reservoir production history. In this study, we develop deep neural networks within the reinforcement learning framework to achieve automated history matching that will reduce engineers’ efforts, human bias, automatically and intelligently explore the parameter space, and remove the need of large set of labeled training data. To that end, a fast-marching-based reservoir simulator is encapsulated as an environment for the proposed reinforcement learning. The deep neural-network-based learning agent interacts with the reservoir simulator within reinforcement learning framework to achieve the automated history matching. Reinforcement learning techniques, such as discrete Deep Q Network and continuous Deep Deterministic Policy Gradients, are used toth, used to train the learning agents. The continuous actions enable the Deep Deterministic Policy Gradients to explore more states at each iteration in a a learning episode; consequently, a better history matching is achieved using this algorithm as compared to Deep Q Network. For simplified dual-target composite reservoir models, the best history-matching performances of the discrete and continuous learning methods in terms of normalized root mean square errors are 0.0447 and 0.0038, respectively. Our study shows that continuous action space achieved by the deep deterministic policy gradient drastically outperforms deep Q network.}
}